{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb5270b",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f90e82",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "**Stochastic Gradient Tree Boost**\n",
    "- Init input\n",
    "    - loss_func = squared_error\n",
    "    - learning_rate = 0.1\n",
    "    - n_esitmators = 100                         \n",
    "        - this is denoted as M in the training pseudocode below\n",
    "    - subsample = 0.6                              \n",
    "        - fraction for subsampling, if subsample<1, it is stochastic\n",
    "    - criterion_func = friedman_mse             \n",
    "        - the function to measure the quality of a split\n",
    "    - max_depth = 3                              \n",
    "        - the maximum depth for each tree\n",
    "    - random_state = None                        \n",
    "        - controls batch, and ....?\n",
    "\n",
    "- Train(X, Y)\n",
    "    - start with an initial guess, say F(x) = mean(Y)\n",
    "    - for m = 1 to M do:\n",
    "        - X_batch, Y_batch = random_sampling(X, Y, fraction for subsampling (batch size))    # fraction < 1 leads to stochastic gradient boosting\n",
    "            - question: do we need to remove the batch from the whole pool at each iteration?\n",
    "        - Y_residual = negative gradient of loss between (true) Y and current F based on the batch\n",
    "        - weak learner h(x) = new weakLearner.fit(X_batch, Y_residual)\n",
    "        - weak learner Tree $\\{R_{lm}\\}_1^L$ = L-terminal node tree fitting on (X_batch, Y_residual)\n",
    "            - here $\\{R_{lm}\\}_1^L$ represents a set of L regions (leaf nodes) in stage m\n",
    "            - $R_{lm}$ represents the l-th region (leaf node) in stage m\n",
    "        - $\\gamma_{lm} = arg min_{\\gamma} \\sum_{x-batch \\in R_{lm}}$ loss(Y_batch, F(x)+$\\gamma$)\n",
    "            - that is, for each region l at stage m, find $\\gamma$ such that it minimizes the loss for the new F(x) = F(x) + $\\gamma$\n",
    "        - update F(x) = F(x) + learning_rate * $\\gamma * 1(x \\in R_{lm})$\n",
    "            - that is, F(x) = F(x) + learning_rate * new tree\n",
    "            - note here we are multiplying the new tree with learning rate (different from how we obtain the best $\\gamma$)\n",
    "    - end for\n",
    "\n",
    "- Predict(X)\n",
    "    - TODO\n",
    "\n",
    "- Loss\n",
    "    - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153bbd12",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd46fab-52db-42da-9903-39a5dcb1b516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a14cffb2",
   "metadata": {},
   "source": [
    "# Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac3cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b347885",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8287d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
