{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb5270b",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391d7c6-1c15-4528-8575-8b3aea1eb1b4",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Our goal is to find a function $F^*(\\mathbf{x})$ that maps $x$ to $y$ such that loss function $\\Psi(y, F(\\mathbf{x}))$ is minimized over the joint distribution of all $(y, x)$ values\n",
    "\n",
    "$$F^*(\\mathbf{x}) = \\arg \\min_{F(\\mathbf{x})} E_{y, \\mathbf{x}} \\, \\Psi(y, F(\\mathbf{x}))$$\n",
    "\n",
    "\n",
    "**Boosting** approximates $F^*(\\mathbf{x})$ with an additive expansion with base learner $h(\\mathbf{x}; \\mathbf{a}_m)$ of the form\n",
    "$$F(\\mathbf{x}) = \\sum_{m=0}^{M} \\beta_m h(\\mathbf{x}; \\mathbf{a}_m)$$\n",
    "where the base learners are usually chosen to be simple functions of $x$ with parameters $a=\\{a_1, a_2, ...\\}$. \n",
    "\n",
    "The expansion coefficients $\\{\\beta_m\\}_0^M$ and the parameters $\\{a_m\\}_0^M$ are jointly fit to the training data in a forward \"stage-wise\" manner. We start with an initial guess $F_0(x)$, and then for stages $m=1,2,...,M$ we find $\\beta_m$, $a_m$ such that the loss between $y$ and $F_m(x)$ is minimized, \n",
    "  $$(\\beta_m, a_m) = \\arg \\min_{\\beta, a} \\sum_{i=1}^{N} \\Psi(y_i, F_{m-1}(x)+\\beta h(x_i;a))$$\n",
    "and $$F_m(x)=F_{m-1}(x)+\\beta_mh(x;a_m)$$.\n",
    "\n",
    "**Gradient Boosting** approximately solves for $\\beta_m$ and $a_m$ with a two step procedure.\n",
    "\n",
    "First, the base learner is fit by least squares \n",
    "    $$a_m = \\arg \\min_{a,p} \\sum_{i=1}^{N}[\\~y_{im}-ph(x_i;a)]^2$$\n",
    "to the current \"pseudo\"-residuals (i.e. negative gradient of the loss w.r.t current predictor)\n",
    "    $$\\~y_{im} = - [\\frac{\\partial \\Psi(y_i, F(x_i))}{\\partial F(x_i)}]_{F(x)=F_{m-1}(x)}$$\n",
    "\n",
    "Second, given the base learner $h(x;a_m)$, the optimal value of the expansion coefficient $\\beta_m$ is determined (i.e. by choosing it such that minimizes the \"total\" loss w.r.t the true $y$ and new predicted value $F_m(x)$)\n",
    "    $$\\beta_m = \\arg \\min_\\beta \\sum_{i=1}^{N} \\Psi(y_i, F_{m-1}(x_i)+\\beta h(x_i;a_m))$$\n",
    "\n",
    "**Gradient Tree Boosting** specializes this approach to the case where the base learner $h(x;a)$ is an L-terminal node regression tree.\n",
    "\n",
    "At each iteration $m$, (*first step*) a regression tree is fitted to the current pseudo-residuals using a top-down \"best-first\" manner with a least-squares splitting criterion. It will partition the $x$-space into L-disjoint regions $\\{R_{lm}\\}_{l=1}^L$ and predicts a separate constant value in each one (region)\n",
    "    $$h(x; \\{R_{lm}\\}_1^L) = \\sum_{l=1}^L \\bar y_{lm} \\mathbb{1}_{x \\in R_{lm}}$$\n",
    "Here the constant value $\\bar y = {mean}_{x_i \\in R_{lm}} (\\~y_{im})$ is the mean of pseudo-residuals in each region $R_{lm}$.\n",
    "The parameters ($a_m$) of this base learner are the splitting variables and corresponding split points defining the tree, which in turn define the corresponding regions $\\{R_{lm}\\}_1^L$ of the partition at the $m \\text{th}$ iteration. \n",
    "\n",
    "Then (*second step*), with the regression tree defined (with $\\{R_{lm}\\}_1^L$ defined), the coefficient $\\beta_m$ can be solved separately within each region $R_{lm}$. Because the tree predicts a constant value within each region, the solution to $\\beta_m$ reduces to a simple \"location\" estimate based on the criterion of the loss function\n",
    "    $$\\gamma_{lm} = \\arg \\min_\\gamma \\sum_{x_i \\in R_{lm}} \\Psi (y_i, F_{m-1}(x_i)+\\gamma)$$\n",
    "\n",
    "The current approximation is then separately updated in each corresponding region\n",
    "    $$F_m(x) = F_{m-1}(x) + lr \\cdot \\gamma_{lm} \\mathbb{1}_{x \\in R_{lm}}$$\n",
    "where $lr$ is the learning rate.\n",
    "\n",
    "**Stochastic Gradient Boosting** draws a subsample of the training data at random (without replacement). This randomly selected subsample is then used to fit the base learner ($a_m$) and compute the model update for the current iteration ($\\beta_m$).\n",
    "\n",
    "Specifically, let $\\{y_i, x_i\\}_1^N$ be the entire training data sample and $\\{\\pi(i)\\}_1^N$ be a random permutation of the integers $\\{1,...,N\\}$. Then, a random subsample of size $\\~N < N$ is given by $\\{y_{\\pi(i)}, x_{\\pi(i)}\\}$.\n",
    "\n",
    "**Our Implementation** takes the loss function of squared error and used stochastic gradient tree boosting for regression. By injecting the randomness of subsampling into the function estimation procedures, it would ideally be less prune to overfitting and more robust. By gradient boosting, it directly fits each base learner to the residual to the negative gradient of the loss thus converges faster and is more efficient. Nevertheless, it still suffers from the following disadvantages more or less -- 1. still potential overfitting, 2. higher computation cost dues to sequential training, 3. harder interpretability due to complex structures of trees invovled with stochastic subsampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebc0d5",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Tree Boosting Pseudocode**\n",
    "- Init input\n",
    "    - learning_rate = 0.1\n",
    "    - n_esitmators = 20                 \n",
    "        - number of estimators (weak learners), same as the number of stages (M)\n",
    "    - subsample = 0.5    \n",
    "        - fraction for subsampling, if subsample<1, it is stochastic\n",
    "    - min_samples = 1\n",
    "        - the minimum number of samples to split for each tree\n",
    "    - max_depth = 3                              \n",
    "        - the maximum depth for each tree\n",
    "\n",
    "- Train(X, Y)\n",
    "    - start with an initial guess, say F(x) = mean(Y)\n",
    "    - for m = 1 to M do:\n",
    "        - X_batch, Y_batch = random_sampling(X, Y, fraction for subsampling (batch size))    # fraction < 1 leads to stochastic gradient boosting\n",
    "        - Y_residual = negative gradient of loss between (true) Y and current F based on the batch\n",
    "            - since we use (half of) sum of squared loss here\n",
    "            - the gradient is thus F_m_batch-Y_batch\n",
    "        - weak learner Tree $\\{R_{lm}\\}_1^L$ = L-terminal node tree fitting on (X_batch, Y_residual)\n",
    "            - here $\\{R_{lm}\\}_1^L$ represents a set of L regions (leaf nodes) in stage m\n",
    "            - $R_{lm}$ represents the l-th region (leaf node) in stage m\n",
    "        - $\\gamma_{lm} = arg min_{\\gamma} \\sum_{x-batch \\in R_{lm}}$ loss(Y_batch, F(x)+$\\gamma$)\n",
    "            - that is, for each region l at stage m, find $\\gamma$ such that it minimizes the loss for the new F(x) = F(x) + $\\gamma$\n",
    "            - which is the mean of the sample values at region l at stage m\n",
    "        - update F(x) = F(x) + learning_rate * $\\gamma * 1(x \\in R_{lm})$\n",
    "            - that is, F(x) = F(x) + learning_rate * new tree\n",
    "            - note here we are multiplying the new tree with learning rate (different from how we obtain the best $\\gamma$)\n",
    "    - end for\n",
    "\n",
    "- Predict(X)\n",
    "    - Given input X, predict their values\n",
    "    - which is the sum of initial guess and all the weak learners' prediction\n",
    "    - where each weak learner's prediction is learning_rate * $\\gamma * 1(x \\in R_{lm})$\n",
    "\n",
    "- Loss(X, Y)\n",
    "    - Given input examples and their true values, calculate the squared loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209555ce",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77f72ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the sum of squared errors for the chosen parameter and splitting point\n",
    "def squaErr(data, result, sequence, parameter, divide):\n",
    "    left = []\n",
    "    right = []\n",
    "\n",
    "    for i in sequence:\n",
    "        if data[i, parameter] < divide:\n",
    "            left.append(i)\n",
    "        else:\n",
    "            right.append(i)\n",
    "\n",
    "    if len(left) == 0 or len(right) == 0:  # If either subset is empty, return positive infinity\n",
    "        return float('inf')\n",
    "\n",
    "    c1 = np.mean(result[left])\n",
    "    err1 = np.sum((result[left] - c1) ** 2)\n",
    "\n",
    "    c2 = np.mean(result[right])\n",
    "    err2 = np.sum((result[right] - c2) ** 2)\n",
    "\n",
    "    return err1 + err2\n",
    "\n",
    "# Determine the next splitting parameter and splitting point using histogram-based binning\n",
    "def bestdivide(data, result, sequence, num_bins=256):\n",
    "    min_para = None\n",
    "    min_divide = None\n",
    "    min_error = float('inf')\n",
    "    \n",
    "    for para in range(data.shape[1]):\n",
    "        # Extract feature values for the current parameter\n",
    "        feature_values = data[sequence, para]\n",
    "        \n",
    "        # Compute histogram bins\n",
    "        bins = np.linspace(feature_values.min(), feature_values.max(), num_bins + 1)\n",
    "        digitized = np.digitize(feature_values, bins) - 1  # Bin indices\n",
    "        \n",
    "        # Bin boundaries as possible split points\n",
    "        bin_boundaries = (bins[:-1] + bins[1:]) / 2  # Midpoints between bin edges\n",
    "        \n",
    "        for boundary in bin_boundaries:\n",
    "            error = squaErr(data, result, sequence, para, boundary)\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                min_para = para\n",
    "                min_divide = boundary\n",
    "\n",
    "    return min_para, min_divide, min_error\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, min_samples=1, max_depth=3):\n",
    "        self.min_samples = min_samples  # Minimum number of samples\n",
    "        self.max_depth = max_depth  # Maximum depth\n",
    "        self.root = None  # Root node of the decision tree\n",
    "\n",
    "    class RegressionTree:\n",
    "        def __init__(self, sequence, depth=0, max_depth=3):\n",
    "            self.isLeaf = True  # Whether this node is a leaf\n",
    "            self.left = None  # Left subtree\n",
    "            self.right = None  # Right subtree\n",
    "            self.output = None  # Prediction value for the current node\n",
    "            self.sequence = sequence  # Indices of samples at the current node\n",
    "            self.parameter = None  # Splitting feature\n",
    "            self.divide = None  # Splitting point\n",
    "            self.depth = depth  # Current depth\n",
    "            self.max_depth = max_depth  # Maximum depth\n",
    "            self.leaf_index = id(self)  # Unique identifier for the leaf\n",
    "\n",
    "        # Grow from the current node\n",
    "        def grow(self, data, result, minnum):\n",
    "            if len(self.sequence) <= minnum or self.depth >= self.max_depth:  # Stop splitting if sample size is insufficient or maximum depth is reached\n",
    "                self.output = np.mean(result[self.sequence])  # Set the prediction value as the mean\n",
    "                return\n",
    "\n",
    "            # Find the best splitting feature and splitting point\n",
    "            parameter, divide, err = bestdivide(data, result, self.sequence)\n",
    "            left = []\n",
    "            right = []\n",
    "\n",
    "            # Split data\n",
    "            for i in self.sequence:\n",
    "                if data[i, parameter] < divide:\n",
    "                    left.append(i)\n",
    "                else:\n",
    "                    right.append(i)\n",
    "\n",
    "            # Update node information\n",
    "            self.parameter = parameter\n",
    "            self.divide = divide\n",
    "            self.isLeaf = False\n",
    "            self.left = DecisionTreeRegressor.RegressionTree(left, depth=self.depth + 1, max_depth=self.max_depth)\n",
    "            self.right = DecisionTreeRegressor.RegressionTree(right, depth=self.depth + 1, max_depth=self.max_depth)\n",
    "\n",
    "            # Recursively grow left and right subtrees\n",
    "            self.left.grow(data, result, minnum)\n",
    "            self.right.grow(data, result, minnum)\n",
    "\n",
    "        # Predict a single sample\n",
    "        def predict_single(self, x):\n",
    "            if self.isLeaf:  # If this is a leaf node, return the prediction value\n",
    "                return self.output\n",
    "            if x[self.parameter] < self.divide:  # If less than the splitting point, go to the left subtree\n",
    "                return self.left.predict_single(x)\n",
    "            else:  # Otherwise, go to the right subtree\n",
    "                return self.right.predict_single(x)\n",
    "            \n",
    "        # Predict leaf index for a single sample\n",
    "        def predict_leaf_index_single(self, x):\n",
    "            if self.isLeaf:  # If this is a leaf node, return the leaf index\n",
    "                return self.leaf_index\n",
    "            if x[self.parameter] < self.divide:  # If less than the splitting point, go to the left subtree\n",
    "                return self.left.predict_leaf_index_single(x)\n",
    "            else:  # Otherwise, go to the right subtree\n",
    "                return self.right.predict_leaf_index_single(x)\n",
    "\n",
    "    # Fit the training data\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.RegressionTree(sequence=range(len(y)), max_depth=self.max_depth)\n",
    "        self.root.grow(X, y, self.min_samples)\n",
    "\n",
    "    # Predict the input data\n",
    "    def predict(self, X):\n",
    "        return np.array([self.root.predict_single(sample) for sample in X])\n",
    "    \n",
    "    # Predict leaf indices for the input data\n",
    "    def predict_leaf_indices(self, X):\n",
    "        return np.array([self.root.predict_leaf_index_single(sample) for sample in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113e4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def squared_loss(predict, target):\n",
    "    '''\n",
    "    Calculates the sum of squared loss between predicted values and true values\n",
    "\n",
    "    :param predict: a 1-d numpy array containing the predicted values\n",
    "    :param target: a 1-d numpy array containing the true values\n",
    "    :return loss: squared loss\n",
    "    '''\n",
    "    loss = 0.5*np.sum(np.power(predict-target, 2))\n",
    "\n",
    "    return loss\n",
    "\n",
    "class StochasticGradientBoosting:\n",
    "    def __init__(self, learning_rate=0.1, n_estimators=20, subsample=0.5, min_samples=1, max_depth=3):\n",
    "        \"\"\"\n",
    "        :param learning_rate: learning rate, default 0.1\n",
    "        :param n_estimators: number of weak learners, default 20 (same as M)\n",
    "        :param subsample: fraction for subsampling, default 0.5\n",
    "        :param min_samples: the minimum number of samples for each tree, default 1\n",
    "        :param max_depth: the maximum depth for each tree (weak learner), default 3\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.subsample = subsample\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []    # models is a list of weak learners (decision trees)\n",
    "        self.gammas = []    # gammas is a list of lists of gamma value for each tree's region\n",
    "        self.initial_prediction = None  # will be initialized in train to be mean\n",
    "        self.leaf_indices_dict = []     # this will store a list of leaf indices for each tree\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        # initial guess F_m=mean(Y)\n",
    "        self.initial_prediction = np.mean(Y)\n",
    "        F_m = np.full(Y.shape, self.initial_prediction) # current F\n",
    "\n",
    "        for m in range(self.n_estimators):\n",
    "            # Random sampling for stochastic gradient boosting\n",
    "            batch_size = math.floor(self.subsample*len(Y))\n",
    "            indices = np.random.choice(len(Y), batch_size, replace=False)\n",
    "            X_batch, Y_batch = X[indices], Y[indices]\n",
    "\n",
    "            # Calculate residuals (negative gradient of the loss)\n",
    "            residuals = Y_batch - F_m[indices]\n",
    "\n",
    "            # Train a weak learner on the residuals\n",
    "            weak_learner = DecisionTreeRegressor(min_samples=self.min_samples, max_depth=self.max_depth)\n",
    "            weak_learner.fit(X_batch, residuals)\n",
    "            self.models.append(weak_learner)\n",
    "\n",
    "            # Update F_m for all samples\n",
    "            leaf_indices = weak_learner.predict_leaf_indices(X)\n",
    "            unique_leaves = np.unique(leaf_indices)\n",
    "            self.leaf_indices_dict.append(unique_leaves)\n",
    "            gamma_m = []    # the gammas for m'th tree, where each region (leaf) will have a corresponding gamma value\n",
    "            for leaf_index in unique_leaves:\n",
    "                region_mask = (leaf_indices == leaf_index)\n",
    "                gamma = np.mean(residuals[region_mask[indices]])\n",
    "                gamma_m.append(gamma)\n",
    "                F_m[region_mask] += self.learning_rate * gamma\n",
    "            self.gammas.append(gamma_m)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Start with the initial prediction\n",
    "        F_m = np.full(X.shape[0], self.initial_prediction)\n",
    "\n",
    "        # Add contributions from each weak learner\n",
    "        for m, model in enumerate(self.models):\n",
    "            leaf_indices = model.predict_leaf_indices(X)\n",
    "            unique_leaves = self.leaf_indices_dict[m]\n",
    "            for i, leaf_index in enumerate(unique_leaves):\n",
    "                region_mask = (leaf_indices == leaf_index)\n",
    "                F_m[region_mask] += self.learning_rate * self.gammas[m][i]\n",
    "\n",
    "        return F_m\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        \"\"\"calculates the squared loss given inputs and their true values\"\"\"\n",
    "        pred = self.predict(X)\n",
    "        loss = squared_loss(pred, Y)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5216db-bee5-4455-ac86-a9c17da00f61",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba1e60-cd61-4a85-b8cc-8b69ae8661cf",
   "metadata": {},
   "source": [
    "Testing individual functions with dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a43a61aa-a20a-407a-a20b-cfbdac04fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "Y = np.array([1.1, 2.0, 2.9, 4.1, 5.0])\n",
    "\n",
    "model = StochasticGradientBoosting(\n",
    "    learning_rate=0.1, \n",
    "    n_estimators=10, \n",
    "    subsample=0.8, \n",
    "    min_samples=1, \n",
    "    max_depth=2\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train(X, Y)\n",
    "\n",
    "# Test initial prediction\n",
    "assert np.isclose(model.initial_prediction, np.mean(Y)), \"Initial prediction should be the mean of Y\"\n",
    "\n",
    "# Test the number of weak learners (decision trees) trained\n",
    "assert len(model.models) == model.n_estimators, \"Number of models should match n_estimators\"\n",
    "\n",
    "# Test that gammas have been computed for each tree\n",
    "assert len(model.gammas) == model.n_estimators, \"Gammas should be computed for each tree\"\n",
    "for gamma_list in model.gammas:\n",
    "    assert len(gamma_list) > 0, \"Each tree should have at least one gamma value\"\n",
    "\n",
    "# Test predictions\n",
    "predictions = model.predict(X)\n",
    "assert predictions.shape == Y.shape, \"Predictions should have the same shape as Y\"\n",
    "\n",
    "# Test if predictions improve with training\n",
    "assert np.mean((predictions - Y) ** 2) < np.mean((np.mean(Y) - Y) ** 2), \\\n",
    "    \"Predictions should reduce mean squared error compared to baseline\"\n",
    "\n",
    "# Test loss \n",
    "loss = model.loss(X, Y)\n",
    "assert loss >= 0, \"Loss should be non-negative\"\n",
    "expected_loss = 0.5 * np.sum(np.power(predictions - Y, 2)) # or mse?\n",
    "assert np.isclose(loss, expected_loss), \"Loss should match the defined loss function (scaled SSE)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5f1e3-c212-440a-9187-f881a31ae29d",
   "metadata": {},
   "source": [
    "Testing against sklearn on dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a2ed4e-1539-4657-91a3-b175710520dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loss: 0.5607269200106376\n",
      "sklearn model loss: 0.38524973632554715\n",
      "model vs sklearn model: 0.1464148251085622\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# toy test for the whole model -- GradientBoostingRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "np.random.seed(1)\n",
    "# set some param for testing\n",
    "learning_rate=0.1\n",
    "n_estimators=50     # generally, more estimators lead to better resutls\n",
    "# subsample=0.5   # subsample of 1 achieves almost the same as sklearn, but subsample less than 1 causes some variation\n",
    "subsample=1\n",
    "min_samples=5\n",
    "max_depth=3\n",
    "\n",
    "def generate_synthetic_data(n_samples=100, noise=0.1):\n",
    "    X = np.random.rand(n_samples, 1) * 10 - 5  # Random features between [-5, 5]\n",
    "    Y = 2 * X.squeeze() + np.sin(X).squeeze() + np.random.randn(n_samples) * noise  # Linear + sine function + noise\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = generate_synthetic_data(n_samples=500)\n",
    "X_test, Y_test = generate_synthetic_data(n_samples=50)\n",
    "\n",
    "# Train the model\n",
    "model = StochasticGradientBoosting(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, min_samples=min_samples, max_depth=max_depth)\n",
    "model.train(X_train, Y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Also the sklearn model\n",
    "sklearn_model = GradientBoostingRegressor(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, min_samples_split=min_samples, max_depth=max_depth)\n",
    "sklearn_model.fit(X_train, Y_train)\n",
    "sklearn_pred = sklearn_model.predict(X_test)\n",
    "sklearn_loss = squared_loss(sklearn_pred, Y_test)\n",
    "\n",
    "# print loss\n",
    "print(f\"model loss: {squared_loss(predictions, Y_test)}\")\n",
    "print(f\"sklearn model loss: {squared_loss(sklearn_pred, Y_test)}\")\n",
    "print(f\"model vs sklearn model: {squared_loss(predictions, sklearn_pred)}\")\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_test, Y_test, color='blue', label='True values')\n",
    "plt.scatter(X_test, predictions, color='red', label='Predicted values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae37af-dfaf-4ece-961b-6418c8ebe3a0",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f31b7-9538-4803-8b98-eba70ac242cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on wine.txt dataset\n",
      "----- StochasticGradientBoosting -----\n",
      "hi\n",
      "My Model Training Loss: 0.378544231509949\n",
      "My Model Testing Loss: 0.5809187798022113\n",
      "----- sklearn GradientBoostingRegressor -----\n",
      "Sklearn Model Training Loss: 0.37438377629098263\n",
      "Sklearn Model Testing Loss: 0.568075903436917\n",
      "----- Comparison -----\n",
      "Training Loss Difference: 0.0042\n",
      "Testing Loss Difference: 0.0128\n"
     ]
    }
   ],
   "source": [
    "# get a public dataset\n",
    "\n",
    "\n",
    "# compare model result on the dataset with sklearn result on the same dataset\n",
    "#X, Y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "\n",
    "\n",
    "def squared_loss(predictions, targets):\n",
    "    \"\"\"Custom squared loss calculation.\"\"\"\n",
    "    return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "def test_boosting_models(dataset, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Compares the performance of our StochasticGradientBoosting with sklearn's GradientBoostingRegressor on a given dataset.\n",
    "    :param dataset: The path to the dataset\n",
    "    :param test_size: The proportion of the dataset to include in the test split\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dataset):\n",
    "        print(f\"The file {dataset} does not exist\")\n",
    "        exit()\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(dataset, skiprows=1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the features\n",
    "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "\n",
    "    print('Running models on {} dataset'.format(dataset))\n",
    "    \n",
    "    #### StochasticGradientBoosting ######\n",
    "    print(\"----- StochasticGradientBoosting -----\")\n",
    "    my_model = StochasticGradientBoosting(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        subsample=0.8,\n",
    "        min_samples=2,\n",
    "        max_depth=3\n",
    "    )\n",
    "    #print(X_train.shape, Y_train.shape)\n",
    "    my_model.train(X_train, Y_train)   # why is this taking so long\n",
    "    print(\"hi\")\n",
    "    my_train_predictions = my_model.predict(X_train)\n",
    "    my_test_predictions = my_model.predict(X_test)\n",
    "\n",
    "    my_train_loss = squared_loss(my_train_predictions, Y_train)\n",
    "    my_test_loss = squared_loss(my_test_predictions, Y_test)\n",
    "\n",
    "    print(\"My Model Training Loss:\", my_train_loss)\n",
    "    print(\"My Model Testing Loss:\", my_test_loss)\n",
    "   \n",
    "    #### sklearn GradientBoostingRegressor ######\n",
    "    print(\"----- sklearn GradientBoostingRegressor -----\")\n",
    "    sklearn_model = GradientBoostingRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        subsample=0.8,\n",
    "        min_samples_split=2,\n",
    "        max_depth=3,\n",
    "        random_state=0\n",
    "    )\n",
    "    sklearn_model.fit(X_train, Y_train)\n",
    "    sklearn_train_predictions = sklearn_model.predict(X_train)\n",
    "    sklearn_test_predictions = sklearn_model.predict(X_test)\n",
    "\n",
    "    sklearn_train_loss = squared_loss(sklearn_train_predictions, Y_train)\n",
    "    sklearn_test_loss = squared_loss(sklearn_test_predictions, Y_test)\n",
    "\n",
    "    print(\"Sklearn Model Training Loss:\", sklearn_train_loss)\n",
    "    print(\"Sklearn Model Testing Loss:\", sklearn_test_loss)\n",
    "\n",
    "    #### Compare results ######\n",
    "    print(\"----- Comparison -----\")\n",
    "    print(f\"Training Loss Difference: {my_train_loss - sklearn_train_loss:.4f}\")\n",
    "    print(f\"Testing Loss Difference: {my_test_loss - sklearn_test_loss:.4f}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "test_boosting_models('wine.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c93604-964b-4b2a-ad9c-0aaae43e040e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbc375-c30a-41b3-9995-b951c0bd4b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
