{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb5270b",
   "metadata": {},
   "source": [
    "# Overview: Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391d7c6-1c15-4528-8575-8b3aea1eb1b4",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Our goal is to find a function $F^*(\\mathbf{x})$ that maps $x$ to $y$ such that loss function $\\Psi(y, F(\\mathbf{x}))$ is minimized over the joint distribution of all $(y, x)$ values\n",
    "\n",
    "$$F^*(\\mathbf{x}) = \\arg \\min_{F(\\mathbf{x})} E_{y, \\mathbf{x}} \\, \\Psi(y, F(\\mathbf{x}))$$\n",
    "\n",
    "\n",
    "Boosting: additive expansion with base learner $h(\\mathbf{x}; \\mathbf{a}_m)$\n",
    "$$F(\\mathbf{x}) = \\sum_{m=0}^{M} \\beta_m h(\\mathbf{x}; \\mathbf{a}_m)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94572767-125c-4697-82a8-9a830bef32da",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting\n",
    "\n",
    "$$F_0(\\mathbf{x}) = \\arg \\min_{\\gamma} \\sum_{i=1}^{N} \\Psi(y_i, \\gamma)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebc0d5",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Tree Boosting Pseudocode**\n",
    "- Init input\n",
    "    - loss_func = squared_error\n",
    "    - learning_rate = 0.1\n",
    "    - n_esitmators = 100                         \n",
    "        - this is denoted as M in the training pseudocode below\n",
    "    - subsample = 0.6                              \n",
    "        - fraction for subsampling, if subsample<1, it is stochastic\n",
    "    - criterion_func = friedman_mse             \n",
    "        - the function to measure the quality of a split\n",
    "    - max_depth = 3                              \n",
    "        - the maximum depth for each tree\n",
    "    - random_state = None                        \n",
    "        - controls batch, and ....?\n",
    "\n",
    "- Train(X, Y)\n",
    "    - start with an initial guess, say F(x) = mean(Y)\n",
    "    - for m = 1 to M do:\n",
    "        - X_batch, Y_batch = random_sampling(X, Y, fraction for subsampling (batch size))    # fraction < 1 leads to stochastic gradient boosting\n",
    "            - question: do we need to remove the batch from the whole pool at each iteration?\n",
    "        - Y_residual = negative gradient of loss between (true) Y and current F based on the batch\n",
    "        - weak learner h(x) = new weakLearner.fit(X_batch, Y_residual)\n",
    "        - weak learner Tree $\\{R_{lm}\\}_1^L$ = L-terminal node tree fitting on (X_batch, Y_residual)\n",
    "            - here $\\{R_{lm}\\}_1^L$ represents a set of L regions (leaf nodes) in stage m\n",
    "            - $R_{lm}$ represents the l-th region (leaf node) in stage m\n",
    "        - $\\gamma_{lm} = arg min_{\\gamma} \\sum_{x-batch \\in R_{lm}}$ loss(Y_batch, F(x)+$\\gamma$)\n",
    "            - that is, for each region l at stage m, find $\\gamma$ such that it minimizes the loss for the new F(x) = F(x) + $\\gamma$\n",
    "        - update F(x) = F(x) + learning_rate * $\\gamma * 1(x \\in R_{lm})$\n",
    "            - that is, F(x) = F(x) + learning_rate * new tree\n",
    "            - note here we are multiplying the new tree with learning rate (different from how we obtain the best $\\gamma$)\n",
    "    - end for\n",
    "\n",
    "- Predict(X)\n",
    "    - TODO\n",
    "\n",
    "- Loss\n",
    "    - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280330d-faee-4c4f-be76-9072c3d3b300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
