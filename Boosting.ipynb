{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb5270b",
   "metadata": {},
   "source": [
    "# Overview: Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391d7c6-1c15-4528-8575-8b3aea1eb1b4",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Our goal is to find a function $F^*(\\mathbf{x})$ that maps $x$ to $y$ such that loss function $\\Psi(y, F(\\mathbf{x}))$ is minimized over the joint distribution of all $(y, x)$ values\n",
    "\n",
    "$$F^*(\\mathbf{x}) = \\arg \\min_{F(\\mathbf{x})} E_{y, \\mathbf{x}} \\, \\Psi(y, F(\\mathbf{x}))$$\n",
    "\n",
    "\n",
    "Boosting: additive expansion with base learner $h(\\mathbf{x}; \\mathbf{a}_m)$\n",
    "$$F(\\mathbf{x}) = \\sum_{m=0}^{M} \\beta_m h(\\mathbf{x}; \\mathbf{a}_m)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94572767-125c-4697-82a8-9a830bef32da",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting\n",
    "\n",
    "$$F_0(\\mathbf{x}) = \\arg \\min_{\\gamma} \\sum_{i=1}^{N} \\Psi(y_i, \\gamma)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebc0d5",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Tree Boosting Pseudocode**\n",
    "- Init input\n",
    "    - learning_rate = 0.1\n",
    "    - n_esitmators = 20                 \n",
    "        - this is denoted as M in the training pseudocode below\n",
    "    - subsample = 0.5    \n",
    "        - fraction for subsampling, if subsample<1, it is stochastic\n",
    "    - min_samples = 1\n",
    "        - the minimum number of samples to split for each tree\n",
    "    - max_depth = 3                              \n",
    "        - the maximum depth for each tree\n",
    "\n",
    "- Train(X, Y)\n",
    "    - start with an initial guess, say F(x) = mean(Y)\n",
    "    - for m = 1 to M do:\n",
    "        - X_batch, Y_batch = random_sampling(X, Y, fraction for subsampling (batch size))    # fraction < 1 leads to stochastic gradient boosting\n",
    "        - Y_residual = negative gradient of loss between (true) Y and current F based on the batch\n",
    "            - since we use (half of) sum of squared loss here\n",
    "            - the gradient is thus F_m_batch-Y_batch\n",
    "        - weak learner h(x) = new weakLearner.fit(X_batch, Y_residual)\n",
    "        - weak learner Tree $\\{R_{lm}\\}_1^L$ = L-terminal node tree fitting on (X_batch, Y_residual)\n",
    "            - here $\\{R_{lm}\\}_1^L$ represents a set of L regions (leaf nodes) in stage m\n",
    "            - $R_{lm}$ represents the l-th region (leaf node) in stage m\n",
    "        - $\\gamma_{lm} = arg min_{\\gamma} \\sum_{x-batch \\in R_{lm}}$ loss(Y_batch, F(x)+$\\gamma$)\n",
    "            - that is, for each region l at stage m, find $\\gamma$ such that it minimizes the loss for the new F(x) = F(x) + $\\gamma$\n",
    "            - which is the mean of the sample values at region l at stage m\n",
    "        - update F(x) = F(x) + learning_rate * $\\gamma * 1(x \\in R_{lm})$\n",
    "            - that is, F(x) = F(x) + learning_rate * new tree\n",
    "            - note here we are multiplying the new tree with learning rate (different from how we obtain the best $\\gamma$)\n",
    "    - end for\n",
    "\n",
    "- Predict(X)\n",
    "    - Given input X, predict their values\n",
    "    - which is the sum of initial guess and all the weak learners' prediction\n",
    "    - where each weak learner's prediction is learning_rate * $\\gamma * 1(x \\in R_{lm})$\n",
    "\n",
    "- Loss(X, Y)\n",
    "    - Given input examples and their true values, calculate the squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b77f72ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, min_samples=1, max_depth=3):\n",
    "        self.min_samples = min_samples  # Minimum number of samples\n",
    "        self.max_depth = max_depth  # Maximum depth\n",
    "        self.root = None  # Root node of the decision tree\n",
    "\n",
    "    class RegressionTree:\n",
    "        def __init__(self, sequence, depth=0, max_depth=3):\n",
    "            self.isLeaf = True  # Whether this node is a leaf\n",
    "            self.left = None  # Left subtree\n",
    "            self.right = None  # Right subtree\n",
    "            self.output = None  # Prediction value for the current node\n",
    "            self.sequence = sequence  # Indices of samples at the current node\n",
    "            self.parameter = None  # Splitting feature\n",
    "            self.divide = None  # Splitting point\n",
    "            self.depth = depth  # Current depth\n",
    "            self.max_depth = max_depth  # Maximum depth\n",
    "            self.leaf_index = id(self)  # Unique identifier for the leaf\n",
    "\n",
    "        # Grow from the current node\n",
    "        def grow(self, data, result, minnum):\n",
    "            if len(self.sequence) <= minnum or self.depth >= self.max_depth:  # Stop splitting if sample size is insufficient or maximum depth is reached\n",
    "                self.output = np.mean(result[self.sequence])  # Set the prediction value as the mean\n",
    "                return\n",
    "\n",
    "            # Find the best splitting feature and splitting point\n",
    "            parameter, divide, err = bestdivide(data, result, self.sequence)\n",
    "            left = []\n",
    "            right = []\n",
    "\n",
    "            # Split data\n",
    "            for i in self.sequence:\n",
    "                if data[i, parameter] < divide:\n",
    "                    left.append(i)\n",
    "                else:\n",
    "                    right.append(i)\n",
    "\n",
    "            # Update node information\n",
    "            self.parameter = parameter\n",
    "            self.divide = divide\n",
    "            self.isLeaf = False\n",
    "            self.left = DecisionTreeRegressor.RegressionTree(left, depth=self.depth + 1, max_depth=self.max_depth)\n",
    "            self.right = DecisionTreeRegressor.RegressionTree(right, depth=self.depth + 1, max_depth=self.max_depth)\n",
    "\n",
    "            # Recursively grow left and right subtrees\n",
    "            self.left.grow(data, result, minnum)\n",
    "            self.right.grow(data, result, minnum)\n",
    "\n",
    "        # Predict a single sample\n",
    "        def predict_single(self, x):\n",
    "            if self.isLeaf:  # If this is a leaf node, return the prediction value\n",
    "                return self.output\n",
    "            if x[self.parameter] < self.divide:  # If less than the splitting point, go to the left subtree\n",
    "                return self.left.predict_single(x)\n",
    "            else:  # Otherwise, go to the right subtree\n",
    "                return self.right.predict_single(x)\n",
    "            \n",
    "        # Predict leaf index for a single sample\n",
    "        def predict_leaf_index_single(self, x):\n",
    "            if self.isLeaf:  # If this is a leaf node, return the leaf index\n",
    "                return self.leaf_index\n",
    "            if x[self.parameter] < self.divide:  # If less than the splitting point, go to the left subtree\n",
    "                return self.left.predict_leaf_index_single(x)\n",
    "            else:  # Otherwise, go to the right subtree\n",
    "                return self.right.predict_leaf_index_single(x)\n",
    "\n",
    "    # Fit the training data\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.RegressionTree(sequence=range(len(y)), max_depth=self.max_depth)\n",
    "        self.root.grow(X, y, self.min_samples)\n",
    "\n",
    "    # Predict the input data\n",
    "    def predict(self, X):\n",
    "        return np.array([self.root.predict_single(sample) for sample in X])\n",
    "    \n",
    "    # Predict leaf indices for the input data\n",
    "    def predict_leaf_indices(self, X):\n",
    "        return np.array([self.root.predict_leaf_index_single(sample) for sample in X])\n",
    "\n",
    "# Calculate the sum of squared errors for the chosen parameter and splitting point\n",
    "def squaErr(data, result, sequence, parameter, divide):\n",
    "    left = []\n",
    "    right = []\n",
    "\n",
    "    for i in sequence:\n",
    "        if data[i, parameter] < divide:\n",
    "            left.append(i)\n",
    "        else:\n",
    "            right.append(i)\n",
    "\n",
    "    if len(left) == 0 or len(right) == 0:  # If either subset is empty, return positive infinity\n",
    "        return float('inf')\n",
    "\n",
    "    c1 = np.mean(result[left])\n",
    "    err1 = np.sum((result[left] - c1) ** 2)\n",
    "\n",
    "    c2 = np.mean(result[right])\n",
    "    err2 = np.sum((result[right] - c2) ** 2)\n",
    "\n",
    "    return err1 + err2\n",
    "\n",
    "# Determine the next splitting parameter and splitting point by exhaustive search\n",
    "def bestdivide(data, result, sequence):\n",
    "    min_para = 0\n",
    "    sortedValue = np.sort(data[sequence][:, min_para])\n",
    "    min_divide = (sortedValue[0] + sortedValue[1]) / 2\n",
    "    err = squaErr(data, result, sequence, min_para, min_divide)\n",
    "\n",
    "    for para in range(data.shape[1]):\n",
    "        sortedValue = np.sort(data[sequence][:, para])\n",
    "        sliceValue = (sortedValue[1:] + sortedValue[:-1]) / 2\n",
    "\n",
    "        for divide in sliceValue:\n",
    "            errNew = squaErr(data, result, sequence, para, divide)\n",
    "            if errNew < err:\n",
    "                err = errNew\n",
    "                min_para = para\n",
    "                min_divide = divide\n",
    "\n",
    "    return min_para, min_divide, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "113e4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def squared_loss(predict, target):\n",
    "    '''\n",
    "    Calculates the sum of squared loss between predicted values and true values\n",
    "\n",
    "    :param predict: a 1-d numpy array containing the predicted values\n",
    "    :param target: a 1-d numpy array containing the true values\n",
    "    :return loss: squared loss\n",
    "    '''\n",
    "    loss = 0.5*np.sum(np.power(predict-target, 2))\n",
    "\n",
    "    return loss\n",
    "\n",
    "class StochasticGradientBoosting:\n",
    "    def __init__(self, learning_rate=0.1, n_estimators=20, subsample=0.5, min_samples=1, max_depth=3):\n",
    "        \"\"\"\n",
    "        :param learning_rate: learning rate, default 0.1\n",
    "        :param n_estimators: number of weak learners, default 20 (same as M)\n",
    "        :param subsample: fraction for subsampling, default 0.5\n",
    "        :param min_samples: the minimum number of samples for each tree, default 1\n",
    "        :param max_depth: the maximum depth for each tree (weak learner), default 3\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.subsample = subsample\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []    # models is a list of weak learners (decision trees)\n",
    "        self.gammas = []    # gammas is a list of lists of gamma value for each tree's region\n",
    "        self.initial_prediction = None  # will be initialized in train to be mean\n",
    "        self.leaf_indices_dict = []     # this will store a list of leaf indices for each tree\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        # initial guess F_m=mean(Y)\n",
    "        self.initial_prediction = np.mean(Y)\n",
    "        F_m = np.full(Y.shape, self.initial_prediction) # current F\n",
    "\n",
    "        for m in range(self.n_estimators):\n",
    "            # Random sampling for stochastic gradient boosting\n",
    "            batch_size = math.floor(self.subsample*len(Y))\n",
    "            indices = np.random.choice(len(Y), batch_size, replace=False)\n",
    "            X_batch, Y_batch = X[indices], Y[indices]\n",
    "\n",
    "            # Calculate residuals (negative gradient of the loss)\n",
    "            residuals = Y_batch - F_m[indices]\n",
    "\n",
    "            # Train a weak learner on the residuals\n",
    "            weak_learner = DecisionTreeRegressor(min_samples=self.min_samples, max_depth=self.max_depth)\n",
    "            weak_learner.fit(X_batch, residuals)\n",
    "            self.models.append(weak_learner)\n",
    "\n",
    "            # Update F_m for all samples\n",
    "            leaf_indices = weak_learner.predict_leaf_indices(X)\n",
    "            unique_leaves = np.unique(leaf_indices)\n",
    "            self.leaf_indices_dict.append(unique_leaves)\n",
    "            gamma_m = []    # the gammas for m'th tree, where each region (leaf) will have a corresponding gamma value\n",
    "            for leaf_index in unique_leaves:\n",
    "                region_mask = (leaf_indices == leaf_index)\n",
    "                gamma = np.mean(residuals[region_mask[indices]])\n",
    "                gamma_m.append(gamma)\n",
    "                F_m[region_mask] += self.learning_rate * gamma\n",
    "            self.gammas.append(gamma_m)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Start with the initial prediction\n",
    "        F_m = np.full(X.shape[0], self.initial_prediction)\n",
    "\n",
    "        # Add contributions from each weak learner\n",
    "        for m, model in enumerate(self.models):\n",
    "            leaf_indices = model.predict_leaf_indices(X)\n",
    "            unique_leaves = self.leaf_indices_dict[m]\n",
    "            for i, leaf_index in enumerate(unique_leaves):\n",
    "                region_mask = (leaf_indices == leaf_index)\n",
    "                F_m[region_mask] += self.learning_rate * self.gammas[m][i]\n",
    "\n",
    "        return F_m\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        \"\"\"calculates the squared loss given inputs and their true values\"\"\"\n",
    "        pred = self.predict(X)\n",
    "        loss = squared_loss(pred, Y)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5216db-bee5-4455-ac86-a9c17da00f61",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba1e60-cd61-4a85-b8cc-8b69ae8661cf",
   "metadata": {},
   "source": [
    "Testing individual functions with dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a43a61aa-a20a-407a-a20b-cfbdac04fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "Y = np.array([1.1, 2.0, 2.9, 4.1, 5.0])\n",
    "\n",
    "model = StochasticGradientBoosting(\n",
    "    learning_rate=0.1, \n",
    "    n_estimators=10, \n",
    "    subsample=0.8, \n",
    "    min_samples=1, \n",
    "    max_depth=2\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train(X, Y)\n",
    "\n",
    "# Test initial prediction\n",
    "assert np.isclose(model.initial_prediction, np.mean(Y)), \"Initial prediction should be the mean of Y\"\n",
    "\n",
    "# Test the number of weak learners (decision trees) trained\n",
    "assert len(model.models) == model.n_estimators, \"Number of models should match n_estimators\"\n",
    "\n",
    "# Test that gammas have been computed for each tree\n",
    "assert len(model.gammas) == model.n_estimators, \"Gammas should be computed for each tree\"\n",
    "for gamma_list in model.gammas:\n",
    "    assert len(gamma_list) > 0, \"Each tree should have at least one gamma value\"\n",
    "\n",
    "# Test predictions\n",
    "predictions = model.predict(X)\n",
    "assert predictions.shape == Y.shape, \"Predictions should have the same shape as Y\"\n",
    "\n",
    "# Test if predictions improve with training\n",
    "assert np.mean((predictions - Y) ** 2) < np.mean((np.mean(Y) - Y) ** 2), \\\n",
    "    \"Predictions should reduce mean squared error compared to baseline\"\n",
    "\n",
    "# Test loss \n",
    "loss = model.loss(X, Y)\n",
    "assert loss >= 0, \"Loss should be non-negative\"\n",
    "expected_loss = 0.5 * np.sum(np.power(predictions - Y, 2)) # or mse?\n",
    "assert np.isclose(loss, expected_loss), \"Loss should match the defined loss function (scaled SSE)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5f1e3-c212-440a-9187-f881a31ae29d",
   "metadata": {},
   "source": [
    "Testing against sklearn on dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28a2ed4e-1539-4657-91a3-b175710520dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loss: 0.38524973632554743\n",
      "sklearn model loss: 0.3852497363255467\n",
      "model vs sklearn model: 1.2382959170432172e-29\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# toy test for the whole model -- GradientBoostingRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "np.random.seed(1)\n",
    "# set some param for testing\n",
    "learning_rate=0.1\n",
    "n_estimators=50     # generally, more estimators lead to better resutls\n",
    "# subsample=0.5   # subsample of 1 achieves almost the same as sklearn, but subsample less than 1 causes some variation\n",
    "subsample=1\n",
    "min_samples=5\n",
    "max_depth=3\n",
    "\n",
    "def generate_synthetic_data(n_samples=100, noise=0.1):\n",
    "    X = np.random.rand(n_samples, 1) * 10 - 5  # Random features between [-5, 5]\n",
    "    Y = 2 * X.squeeze() + np.sin(X).squeeze() + np.random.randn(n_samples) * noise  # Linear + sine function + noise\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = generate_synthetic_data(n_samples=500)\n",
    "X_test, Y_test = generate_synthetic_data(n_samples=50)\n",
    "\n",
    "# Train the model\n",
    "model = StochasticGradientBoosting(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, min_samples=min_samples, max_depth=max_depth)\n",
    "model.train(X_train, Y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Also the sklearn model\n",
    "sklearn_model = GradientBoostingRegressor(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, min_samples_split=min_samples, max_depth=max_depth)\n",
    "sklearn_model.fit(X_train, Y_train)\n",
    "sklearn_pred = sklearn_model.predict(X_test)\n",
    "sklearn_loss = squared_loss(sklearn_pred, Y_test)\n",
    "\n",
    "# print loss\n",
    "print(f\"model loss: {squared_loss(predictions, Y_test)}\")\n",
    "print(f\"sklearn model loss: {squared_loss(sklearn_pred, Y_test)}\")\n",
    "print(f\"model vs sklearn model: {squared_loss(predictions, sklearn_pred)}\")\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_test, Y_test, color='blue', label='True values')\n",
    "plt.scatter(X_test, predictions, color='red', label='Predicted values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae37af-dfaf-4ece-961b-6418c8ebe3a0",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f31b7-9538-4803-8b98-eba70ac242cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on wine.txt dataset\n",
      "----- StochasticGradientBoosting -----\n",
      "(3918, 11) (3918,)\n"
     ]
    }
   ],
   "source": [
    "# get a public dataset\n",
    "\n",
    "\n",
    "# compare model result on the dataset with sklearn result on the same dataset\n",
    "#X, Y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "\n",
    "\n",
    "def squared_loss(predictions, targets):\n",
    "    \"\"\"Custom squared loss calculation.\"\"\"\n",
    "    return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "def test_boosting_models(dataset, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Compares the performance of our StochasticGradientBoosting with sklearn's GradientBoostingRegressor on a given dataset.\n",
    "    :param dataset: The path to the dataset\n",
    "    :param test_size: The proportion of the dataset to include in the test split\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dataset):\n",
    "        print(f\"The file {dataset} does not exist\")\n",
    "        exit()\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(dataset, skiprows=1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the features\n",
    "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "\n",
    "    print('Running models on {} dataset'.format(dataset))\n",
    "    \n",
    "    #### StochasticGradientBoosting ######\n",
    "    print(\"----- StochasticGradientBoosting -----\")\n",
    "    my_model = StochasticGradientBoosting(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        subsample=0.8,\n",
    "        min_samples=2,\n",
    "        max_depth=3\n",
    "    )\n",
    "    #print(X_train.shape, Y_train.shape)\n",
    "    my_model.train(X_train, Y_train)   # why is this taking so long\n",
    "    print(\"hi\")\n",
    "    my_train_predictions = my_model.predict(X_train)\n",
    "    my_test_predictions = my_model.predict(X_test)\n",
    "\n",
    "    my_train_loss = squared_loss(my_train_predictions, Y_train)\n",
    "    my_test_loss = squared_loss(my_test_predictions, Y_test)\n",
    "\n",
    "    print(\"My Model Training Loss:\", my_train_loss)\n",
    "    print(\"My Model Testing Loss:\", my_test_loss)\n",
    "   \n",
    "    #### sklearn GradientBoostingRegressor ######\n",
    "    print(\"----- sklearn GradientBoostingRegressor -----\")\n",
    "    sklearn_model = GradientBoostingRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        subsample=0.8,\n",
    "        min_samples_split=2,\n",
    "        max_depth=3,\n",
    "        random_state=0\n",
    "    )\n",
    "    sklearn_model.fit(X_train, Y_train)\n",
    "    sklearn_train_predictions = sklearn_model.predict(X_train)\n",
    "    sklearn_test_predictions = sklearn_model.predict(X_test)\n",
    "\n",
    "    sklearn_train_loss = squared_loss(sklearn_train_predictions, Y_train)\n",
    "    sklearn_test_loss = squared_loss(sklearn_test_predictions, Y_test)\n",
    "\n",
    "    print(\"Sklearn Model Training Loss:\", sklearn_train_loss)\n",
    "    print(\"Sklearn Model Testing Loss:\", sklearn_test_loss)\n",
    "\n",
    "    #### Compare results ######\n",
    "    print(\"----- Comparison -----\")\n",
    "    print(f\"Training Loss Difference: {my_train_loss - sklearn_train_loss:.4f}\")\n",
    "    print(f\"Testing Loss Difference: {my_test_loss - sklearn_test_loss:.4f}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "test_boosting_models('wine.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c93604-964b-4b2a-ad9c-0aaae43e040e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
