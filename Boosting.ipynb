{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb5270b",
   "metadata": {},
   "source": [
    "# Overview: Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391d7c6-1c15-4528-8575-8b3aea1eb1b4",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Our goal is to find a function $F^*(\\mathbf{x})$ that maps $x$ to $y$ such that loss function $\\Psi(y, F(\\mathbf{x}))$ is minimized over the joint distribution of all $(y, x)$ values\n",
    "\n",
    "$$F^*(\\mathbf{x}) = \\arg \\min_{F(\\mathbf{x})} E_{y, \\mathbf{x}} \\, \\Psi(y, F(\\mathbf{x}))$$\n",
    "\n",
    "\n",
    "Boosting: additive expansion with base learner $h(\\mathbf{x}; \\mathbf{a}_m)$\n",
    "$$F(\\mathbf{x}) = \\sum_{m=0}^{M} \\beta_m h(\\mathbf{x}; \\mathbf{a}_m)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94572767-125c-4697-82a8-9a830bef32da",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting\n",
    "\n",
    "$$F_0(\\mathbf{x}) = \\arg \\min_{\\gamma} \\sum_{i=1}^{N} \\Psi(y_i, \\gamma)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebc0d5",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Tree Boosting Pseudocode**\n",
    "- Init input\n",
    "    - learning_rate = 0.1\n",
    "    - n_esitmators = 20                 \n",
    "        - this is denoted as M in the training pseudocode below\n",
    "    - subsample = 0.5    \n",
    "        - fraction for subsampling, if subsample<1, it is stochastic\n",
    "    - min_samples = 1\n",
    "        - the minimum number of samples to split for each tree\n",
    "    - max_depth = 3                              \n",
    "        - the maximum depth for each tree\n",
    "\n",
    "- Train(X, Y)\n",
    "    - start with an initial guess, say F(x) = mean(Y)\n",
    "    - for m = 1 to M do:\n",
    "        - X_batch, Y_batch = random_sampling(X, Y, fraction for subsampling (batch size))    # fraction < 1 leads to stochastic gradient boosting\n",
    "        - Y_residual = negative gradient of loss between (true) Y and current F based on the batch\n",
    "            - since we use (half of) sum of squared loss here\n",
    "            - the gradient is thus F_m_batch-Y_batch\n",
    "        - weak learner h(x) = new weakLearner.fit(X_batch, Y_residual)\n",
    "        - weak learner Tree $\\{R_{lm}\\}_1^L$ = L-terminal node tree fitting on (X_batch, Y_residual)\n",
    "            - here $\\{R_{lm}\\}_1^L$ represents a set of L regions (leaf nodes) in stage m\n",
    "            - $R_{lm}$ represents the l-th region (leaf node) in stage m\n",
    "        - $\\gamma_{lm} = arg min_{\\gamma} \\sum_{x-batch \\in R_{lm}}$ loss(Y_batch, F(x)+$\\gamma$)\n",
    "            - that is, for each region l at stage m, find $\\gamma$ such that it minimizes the loss for the new F(x) = F(x) + $\\gamma$\n",
    "            - which is the mean of the sample values at region l at stage m\n",
    "        - update F(x) = F(x) + learning_rate * $\\gamma * 1(x \\in R_{lm})$\n",
    "            - that is, F(x) = F(x) + learning_rate * new tree\n",
    "            - note here we are multiplying the new tree with learning rate (different from how we obtain the best $\\gamma$)\n",
    "    - end for\n",
    "\n",
    "- Predict(X)\n",
    "    - Given input X, predict their values\n",
    "    - which is the sum of initial guess and all the weak learners' prediction\n",
    "    - where each weak learner's prediction is learning_rate * $\\gamma * 1(x \\in R_{lm})$\n",
    "\n",
    "- Loss(X, Y)\n",
    "    - Given input examples and their true values, calculate the squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77f72ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, min_samples=1, max_depth=3):\n",
    "        self.min_samples = min_samples  # Minimum number of samples\n",
    "        self.max_depth = max_depth  # Maximum depth\n",
    "        self.root = None  # Root node of the decision tree\n",
    "\n",
    "    class RegressionTree:\n",
    "        def __init__(self, sequence, depth=0, max_depth=3):\n",
    "            self.isLeaf = True  # Whether this node is a leaf\n",
    "            self.left = None  # Left subtree\n",
    "            self.right = None  # Right subtree\n",
    "            self.output = None  # Prediction value for the current node\n",
    "            self.sequence = sequence  # Indices of samples at the current node\n",
    "            self.parameter = None  # Splitting feature\n",
    "            self.divide = None  # Splitting point\n",
    "            self.depth = depth  # Current depth\n",
    "            self.max_depth = max_depth  # Maximum depth\n",
    "            self.leaf_index = id(self)  # Unique identifier for the leaf\n",
    "\n",
    "        # Grow from the current node\n",
    "        def grow(self, data, result, minnum):\n",
    "            if len(self.sequence) <= minnum or self.depth >= self.max_depth:  # Stop splitting if sample size is insufficient or maximum depth is reached\n",
    "                self.output = np.mean(result[self.sequence])  # Set the prediction value as the mean\n",
    "                return\n",
    "\n",
    "            # Find the best splitting feature and splitting point\n",
    "            parameter, divide, err = bestdivide(data, result, self.sequence)\n",
    "            left = []\n",
    "            right = []\n",
    "\n",
    "            # Split data\n",
    "            for i in self.sequence:\n",
    "                if data[i, parameter] < divide:\n",
    "                    left.append(i)\n",
    "                else:\n",
    "                    right.append(i)\n",
    "\n",
    "            # Update node information\n",
    "            self.parameter = parameter\n",
    "            self.divide = divide\n",
    "            self.isLeaf = False\n",
    "            self.left = DecisionTreeRegressor.RegressionTree(left, depth=self.depth + 1, max_depth=self.max_depth)\n",
    "            self.right = DecisionTreeRegressor.RegressionTree(right, depth=self.depth + 1, max_depth=self.max_depth)\n",
    "\n",
    "            # Recursively grow left and right subtrees\n",
    "            self.left.grow(data, result, minnum)\n",
    "            self.right.grow(data, result, minnum)\n",
    "\n",
    "        # Predict a single sample\n",
    "        def predict_single(self, x):\n",
    "            if self.isLeaf:  # If this is a leaf node, return the prediction value\n",
    "                return self.output\n",
    "            if x[self.parameter] < self.divide:  # If less than the splitting point, go to the left subtree\n",
    "                return self.left.predict_single(x)\n",
    "            else:  # Otherwise, go to the right subtree\n",
    "                return self.right.predict_single(x)\n",
    "            \n",
    "        # Predict leaf index for a single sample\n",
    "        def predict_leaf_index_single(self, x):\n",
    "            if self.isLeaf:  # If this is a leaf node, return the leaf index\n",
    "                return self.leaf_index\n",
    "            if x[self.parameter] < self.divide:  # If less than the splitting point, go to the left subtree\n",
    "                return self.left.predict_leaf_index_single(x)\n",
    "            else:  # Otherwise, go to the right subtree\n",
    "                return self.right.predict_leaf_index_single(x)\n",
    "\n",
    "    # Fit the training data\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.RegressionTree(sequence=range(len(y)), max_depth=self.max_depth)\n",
    "        self.root.grow(X, y, self.min_samples)\n",
    "\n",
    "    # Predict the input data\n",
    "    def predict(self, X):\n",
    "        return np.array([self.root.predict_single(sample) for sample in X])\n",
    "    \n",
    "    # Predict leaf indices for the input data\n",
    "    def predict_leaf_indices(self, X):\n",
    "        return np.array([self.root.predict_leaf_index_single(sample) for sample in X])\n",
    "\n",
    "# Calculate the sum of squared errors for the chosen parameter and splitting point\n",
    "def squaErr(data, result, sequence, parameter, divide):\n",
    "    left = []\n",
    "    right = []\n",
    "\n",
    "    for i in sequence:\n",
    "        if data[i, parameter] < divide:\n",
    "            left.append(i)\n",
    "        else:\n",
    "            right.append(i)\n",
    "\n",
    "    if len(left) == 0 or len(right) == 0:  # If either subset is empty, return positive infinity\n",
    "        return float('inf')\n",
    "\n",
    "    c1 = np.mean(result[left])\n",
    "    err1 = np.sum((result[left] - c1) ** 2)\n",
    "\n",
    "    c2 = np.mean(result[right])\n",
    "    err2 = np.sum((result[right] - c2) ** 2)\n",
    "\n",
    "    return err1 + err2\n",
    "\n",
    "# Determine the next splitting parameter and splitting point by exhaustive search\n",
    "def bestdivide(data, result, sequence):\n",
    "    min_para = 0\n",
    "    sortedValue = np.sort(data[sequence][:, min_para])\n",
    "    min_divide = (sortedValue[0] + sortedValue[1]) / 2\n",
    "    err = squaErr(data, result, sequence, min_para, min_divide)\n",
    "\n",
    "    for para in range(data.shape[1]):\n",
    "        sortedValue = np.sort(data[sequence][:, para])\n",
    "        sliceValue = (sortedValue[1:] + sortedValue[:-1]) / 2\n",
    "\n",
    "        for divide in sliceValue:\n",
    "            errNew = squaErr(data, result, sequence, para, divide)\n",
    "            if errNew < err:\n",
    "                err = errNew\n",
    "                min_para = para\n",
    "                min_divide = divide\n",
    "\n",
    "    return min_para, min_divide, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113e4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def squared_loss(predict, target):\n",
    "    '''\n",
    "    Calculates the sum of squared loss between predicted values and true values\n",
    "\n",
    "    :param predict: a 1-d numpy array containing the predicted values\n",
    "    :param target: a 1-d numpy array containing the true values\n",
    "    :return loss: squared loss\n",
    "    '''\n",
    "    loss = 0.5*np.sum(np.power(predict-target, 2))\n",
    "\n",
    "    return loss\n",
    "\n",
    "class StochasticGradientBoosting:\n",
    "    def __init__(self, learning_rate=0.1, n_estimators=20, subsample=0.5, min_samples=1, max_depth=3):\n",
    "        \"\"\"\n",
    "        :param learning_rate: learning rate, default 0.1\n",
    "        :param n_estimators: number of weak learners, default 20 (same as M)\n",
    "        :param subsample: fraction for subsampling, default 0.5\n",
    "        :param min_samples: the minimum number of samples for each tree, default 1\n",
    "        :param max_depth: the maximum depth for each tree (weak learner), default 3\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.subsample = subsample\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []    # models is a list of weak learners (decision trees)\n",
    "        self.gammas = []    # gammas is a list of lists of gamma value for each tree's region\n",
    "        self.initial_prediction = None  # will be initialized in train to be mean\n",
    "        self.leaf_indices_dict = []     # this will store a list of leaf indices for each tree\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        # initial guess F_m=mean(Y)\n",
    "        self.initial_prediction = np.mean(Y)\n",
    "        F_m = np.full(Y.shape, self.initial_prediction) # current F\n",
    "\n",
    "        for m in range(self.n_estimators):\n",
    "            # Random sampling for stochastic gradient boosting\n",
    "            batch_size = math.floor(self.subsample*len(Y))\n",
    "            indices = np.random.choice(len(Y), batch_size, replace=False)\n",
    "            X_batch, Y_batch = X[indices], Y[indices]\n",
    "\n",
    "            # Calculate residuals (negative gradient of the loss)\n",
    "            residuals = Y_batch - F_m[indices]\n",
    "\n",
    "            # Train a weak learner on the residuals\n",
    "            weak_learner = DecisionTreeRegressor(min_samples=self.min_samples, max_depth=self.max_depth)\n",
    "            weak_learner.fit(X_batch, residuals)\n",
    "            self.models.append(weak_learner)\n",
    "\n",
    "            # Update F_m for all samples\n",
    "            leaf_indices = weak_learner.predict_leaf_indices(X)\n",
    "            unique_leaves = np.unique(leaf_indices)\n",
    "            self.leaf_indices_dict.append(unique_leaves)\n",
    "            gamma_m = []    # the gammas for m'th tree, where each region (leaf) will have a corresponding gamma value\n",
    "            for leaf_index in unique_leaves:\n",
    "                region_mask = (leaf_indices == leaf_index)\n",
    "                gamma = np.mean(residuals[region_mask[indices]])\n",
    "                gamma_m.append(gamma)\n",
    "                F_m[region_mask] += self.learning_rate * gamma\n",
    "            self.gammas.append(gamma_m)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Start with the initial prediction\n",
    "        F_m = np.full(X.shape[0], self.initial_prediction)\n",
    "\n",
    "        # Add contributions from each weak learner\n",
    "        for m, model in enumerate(self.models):\n",
    "            leaf_indices = model.predict_leaf_indices(X)\n",
    "            unique_leaves = self.leaf_indices_dict[m]\n",
    "            for i, leaf_index in enumerate(unique_leaves):\n",
    "                region_mask = (leaf_indices == leaf_index)\n",
    "                F_m[region_mask] += self.learning_rate * self.gammas[m][i]\n",
    "\n",
    "        return F_m\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        \"\"\"calculates the squared loss given inputs and their true values\"\"\"\n",
    "        pred = self.predict(X)\n",
    "        loss = squared_loss(pred, Y)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a2ed4e-1539-4657-91a3-b175710520dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loss: 0.3852497363255473\n",
      "sklearn model loss: 0.38524973632554715\n",
      "model vs sklearn model: 5.954975388045333e-30\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# toy test for the whole model -- GradientBoostingRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "np.random.seed(1)\n",
    "# set some param for testing\n",
    "learning_rate=0.1\n",
    "n_estimators=50     # generally, more estimators lead to better resutls\n",
    "# subsample=0.5   # subsample of 1 achieves almost the same as sklearn, but subsample less than 1 causes some variation\n",
    "subsample=1\n",
    "min_samples=5\n",
    "max_depth=3\n",
    "\n",
    "def generate_synthetic_data(n_samples=100, noise=0.1):\n",
    "    X = np.random.rand(n_samples, 1) * 10 - 5  # Random features between [-5, 5]\n",
    "    Y = 2 * X.squeeze() + np.sin(X).squeeze() + np.random.randn(n_samples) * noise  # Linear + sine function + noise\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = generate_synthetic_data(n_samples=500)\n",
    "X_test, Y_test = generate_synthetic_data(n_samples=50)\n",
    "\n",
    "# Train the model\n",
    "model = StochasticGradientBoosting(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, min_samples=min_samples, max_depth=max_depth)\n",
    "model.train(X_train, Y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Also the sklearn model\n",
    "sklearn_model = GradientBoostingRegressor(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, min_samples_split=min_samples, max_depth=max_depth)\n",
    "sklearn_model.fit(X_train, Y_train)\n",
    "sklearn_pred = sklearn_model.predict(X_test)\n",
    "sklearn_loss = squared_loss(sklearn_pred, Y_test)\n",
    "\n",
    "# print loss\n",
    "print(f\"model loss: {squared_loss(predictions, Y_test)}\")\n",
    "print(f\"sklearn model loss: {squared_loss(sklearn_pred, Y_test)}\")\n",
    "print(f\"model vs sklearn model: {squared_loss(predictions, sklearn_pred)}\")\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_test, Y_test, color='blue', label='True values')\n",
    "plt.scatter(X_test, predictions, color='red', label='Predicted values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5216db-bee5-4455-ac86-a9c17da00f61",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a43a61aa-a20a-407a-a20b-cfbdac04fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "# testing basic functionalities with some dummy data\n",
    "def generate_data():\n",
    "    np.random.seed(0)\n",
    "    X = np.random.rand(100, 10)\n",
    "    Y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.normal(0, 0.1, 100)  #feature 1 weight 3, feature 2 weight 2\n",
    "    return X, Y\n",
    "    \n",
    "def test_train(generate_data):\n",
    "    X, Y = generate_data\n",
    "    model = StochasticGradientBoosting(n_estimators=10, random_state=0)\n",
    "    model.train(X, Y)\n",
    "    assert len(model.models) == 10\n",
    "    \n",
    "\n",
    "def test_loss(generate_data):\n",
    "    X, Y = generate_data\n",
    "    model = StochasticGradientBoosting(random_state=0)\n",
    "    #Y_pred = np.full_like(Y, np.mean(Y))  # using mean here as initial prediction (?)\n",
    "    # loss = model.loss(Y, Y_pred)\n",
    "    #assert loss > 0 \n",
    "\n",
    "\n",
    "\n",
    "# get a public dataset\n",
    "\n",
    "\n",
    "# compare model result on the dataset with sklearn result on the same dataset\n",
    "#X, Y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
