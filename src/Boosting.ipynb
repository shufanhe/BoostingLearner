{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb5270b",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17721b11-fdaf-499b-93b3-38f8592bdd01",
   "metadata": {},
   "source": [
    "### Names: Shufan He, Chenrui Zhang, Haosheng Wang\n",
    "### Link to the Github repo: https://github.com/shufanhe/BoostingLearner.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391d7c6-1c15-4528-8575-8b3aea1eb1b4",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "All the content bellow follow the method proposed by Jerome H. Friedman's paper on *Gradient Boosting* *(Friedman, 2002)*.\n",
    "\n",
    "Our goal is to find a function $F^*(\\mathbf{x})$ that maps $x$ to $y$ such that loss function $\\Psi(y, F(\\mathbf{x}))$ is minimized over the joint distribution of all $(y, x)$ values\n",
    "\n",
    "$$F^*(\\mathbf{x}) = \\arg \\min_{F(\\mathbf{x})} E_{y, \\mathbf{x}} \\, \\Psi(y, F(\\mathbf{x}))$$\n",
    "\n",
    "\n",
    "**Boosting** approximates $F^*(\\mathbf{x})$ with an additive expansion with base learner $h(\\mathbf{x}; \\mathbf{a}_m)$ of the form\n",
    "$$F(\\mathbf{x}) = \\sum_{m=0}^{M} \\beta_m h(\\mathbf{x}; \\mathbf{a}_m)$$\n",
    "where the base learners are usually chosen to be simple functions of $x$ with parameters $a=\\{a_1, a_2, ...\\}$. \n",
    "\n",
    "The expansion coefficients $\\{\\beta_m\\}_0^M$ and the parameters $\\{a_m\\}_0^M$ are jointly fit to the training data in a forward \"stage-wise\" manner. We start with an initial guess $F_0(x)$, and then for stages $m=1,2,...,M$ we find $\\beta_m$, $a_m$ such that the loss between $y$ and $F_m(x)$ is minimized, \n",
    "  $$(\\beta_m, a_m) = \\arg \\min_{\\beta, a} \\sum_{i=1}^{N} \\Psi(y_i, F_{m-1}(x)+\\beta h(x_i;a))$$\n",
    "and $$F_m(x)=F_{m-1}(x)+\\beta_mh(x;a_m)$$.\n",
    "\n",
    "**Gradient Boosting** approximately solves for $\\beta_m$ and $a_m$ with a two step procedure.\n",
    "\n",
    "First, the base learner is fit by least squares \n",
    "    $$a_m = \\arg \\min_{a,p} \\sum_{i=1}^{N}[\\acute y_{im}-ph(x_i;a)]^2$$\n",
    "to the current \"pseudo\"-residuals (i.e. negative gradient of the loss w.r.t current predictor)\n",
    "    $$\\acute y_{im} = - [\\frac{\\partial \\Psi(y_i, F(x_i))}{\\partial F(x_i)}]_{F(x)=F_{m-1}(x)}$$\n",
    "\n",
    "Second, given the base learner $h(x;a_m)$, the optimal value of the expansion coefficient $\\beta_m$ is determined (i.e. by choosing it such that minimizes the \"total\" loss w.r.t the true $y$ and new predicted value $F_m(x)$)\n",
    "    $$\\beta_m = \\arg \\min_\\beta \\sum_{i=1}^{N} \\Psi(y_i, F_{m-1}(x_i)+\\beta h(x_i;a_m))$$\n",
    "\n",
    "**Gradient Tree Boosting** specializes this approach to the case where the base learner $h(x;a)$ is an L-terminal node regression tree.\n",
    "\n",
    "At each iteration $m$, (*first step*) a regression tree is fitted to the current pseudo-residuals using a top-down \"best-first\" manner with a least-squares splitting criterion. It will partition the $x$-space into L-disjoint regions $\\{R_{lm}\\}_{l=1}^L$ and predicts a separate constant value in each one (region)\n",
    "    $$h(x; \\{R_{lm}\\}_1^L) = \\sum_{l=1}^L \\bar y_{lm} \\mathbb{1}_{x \\in R_{lm}}$$\n",
    "Here the constant value $\\bar y = {mean}_{x_i \\in R_{lm}} (\\acute y_{im})$ is the mean of pseudo-residuals in each region $R_{lm}$.\n",
    "The parameters ($a_m$) of this base learner are the splitting variables and corresponding split points defining the tree, which in turn define the corresponding regions $\\{R_{lm}\\}_1^L$ of the partition at the $m \\text{th}$ iteration. \n",
    "\n",
    "Then (*second step*), with the regression tree defined (with $\\{R_{lm}\\}_1^L$ defined), the coefficient $\\beta_m$ can be solved separately within each region $R_{lm}$. Because the tree predicts a constant value within each region, the solution to $\\beta_m$ reduces to a simple \"location\" estimate based on the criterion of the loss function\n",
    "    $$\\gamma_{lm} = \\arg \\min_\\gamma \\sum_{x_i \\in R_{lm}} \\Psi (y_i, F_{m-1}(x_i)+\\gamma)$$\n",
    "\n",
    "The current approximation is then separately updated in each corresponding region\n",
    "    $$F_m(x) = F_{m-1}(x) + lr \\cdot \\gamma_{lm} \\mathbb{1}_{x \\in R_{lm}}$$\n",
    "where $lr$ is the learning rate.\n",
    "\n",
    "**Stochastic Gradient Boosting** draws a subsample of the training data at random (without replacement). This randomly selected subsample is then used to fit the base learner ($a_m$) and compute the model update for the current iteration ($\\beta_m$).\n",
    "\n",
    "Specifically, let $\\{y_i, x_i\\}_1^N$ be the entire training data sample and $\\{\\pi(i)\\}_1^N$ be a random permutation of the integers $\\{1,...,N\\}$. Then, a random subsample of size $\\acute N < N$ is given by $\\{y_{\\pi(i)}, x_{\\pi(i)}\\}$.\n",
    "\n",
    "**Our Implementation** takes the *loss function* of (half) squared loss (i.e. $0.5*\\sum_{i=1}^N{(y_i - F(x_i))^2}$) and used stochastic gradient tree boosting for regression (*representation* is described above -- as stochastic gradeint tree boosting). The *optimizer* is described in the pseudo code for stochastic gradient tree boosting below. \n",
    "\n",
    "(Advantages) By injecting the randomness of subsampling into the function estimation procedures, it would be less prune to overfitting and more robust. By gradient boosting, it directly fits each base learner to the residual to the negative gradient of the loss thus converges faster and is more efficient. \n",
    "\n",
    "(Disadvantages) Nevertheless, it still suffers from the following disadvantages more or less -- 1. still potential overfitting, 2. higher computation cost dues to sequential training, 3. harder interpretability due to complex structures of trees invovled with stochastic subsampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebc0d5",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Tree Boosting Pseudocode\n",
    "- Init input\n",
    "    - learning_rate = 0.1\n",
    "    - n_esitmators = 20                 \n",
    "        - number of estimators (weak learners), same as the number of stages (M)\n",
    "    - subsample = 0.5    \n",
    "        - fraction for subsampling, if subsample<1, it is stochastic\n",
    "    - min_samples = 1\n",
    "        - the minimum number of samples to split for each tree\n",
    "    - max_depth = 3                              \n",
    "        - the maximum depth for each tree\n",
    "\n",
    "- Train(X, Y)\n",
    "    - start with an initial guess, say F(x) = mean(Y)\n",
    "    - for m = 1 to M do:\n",
    "        - X_batch, Y_batch = random_sampling(X, Y, fraction for subsampling (batch size))    # fraction < 1 leads to stochastic gradient boosting\n",
    "        - Y_residual = negative gradient of loss between (true) Y and current F based on the batch\n",
    "            - since we use (half of) sum of squared loss here\n",
    "            - the gradient is thus F_m_batch-Y_batch\n",
    "        - weak learner Tree $\\{R_{lm}\\}_1^L$ = L-terminal node tree fitting on (X_batch, Y_residual)\n",
    "            - here $\\{R_{lm}\\}_1^L$ represents a set of L regions (leaf nodes) in stage m\n",
    "            - $R_{lm}$ represents the l-th region (leaf node) in stage m\n",
    "        - $\\gamma_{lm} = arg min_{\\gamma} \\sum_{x-batch \\in R_{lm}}$ loss(Y_batch, F(x)+$\\gamma$)\n",
    "            - that is, for each region l at stage m, find $\\gamma$ such that it minimizes the loss for the new F(x) = F(x) + $\\gamma$\n",
    "            - which is the mean of the sample values at region l at stage m\n",
    "        - update F(x) = F(x) + learning_rate * $\\gamma * 1(x \\in R_{lm})$\n",
    "            - that is, F(x) = F(x) + learning_rate * new tree\n",
    "            - note here we are multiplying the new tree with learning rate (different from how we obtain the best $\\gamma$)\n",
    "    - end for\n",
    "\n",
    "- Predict(X)\n",
    "    - Given input X, predict their values\n",
    "    - which is the sum of initial guess and all the weak learners' prediction\n",
    "    - where each weak learner's prediction is learning_rate * $\\gamma * 1(x \\in R_{lm})$\n",
    "\n",
    "- Loss(X, Y)\n",
    "    - Given input examples and their true values, calculate the squared loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209555ce",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77f72ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "def squaErr(data, result, sequence, parameter, divide):\n",
    "    \"\"\"\n",
    "    This function computes the sum of squared errors by splitting the data \n",
    "    based on the specified parameter and dividing point. \n",
    "\n",
    "    :param data : numpy.ndarray\n",
    "        The dataset with samples as rows and features as columns.\n",
    "    :param result : numpy.ndarray\n",
    "        The target values corresponding to the data.\n",
    "    :param sequence : list or numpy.ndarray\n",
    "        Indices of the data points to consider for the split.\n",
    "    :param parameter : int\n",
    "        The index of the feature to split on.\n",
    "    :param divide : float\n",
    "        The value used to split the data for the parameter.\n",
    "\n",
    "    :return err1+err2 : float\n",
    "        The sum of squared errors for the split.\n",
    "    \"\"\"\n",
    "    left = []\n",
    "    right = []\n",
    "\n",
    "    for i in sequence:\n",
    "        if data[i, parameter] < divide:\n",
    "            left.append(i)\n",
    "        else:\n",
    "            right.append(i)\n",
    "\n",
    "    if len(left) == 0 or len(right) == 0:  # If either subset is empty, return positive infinity\n",
    "        return float('inf')\n",
    "\n",
    "    c1 = np.mean(result[left])\n",
    "    err1 = np.sum((result[left] - c1) ** 2)\n",
    "\n",
    "    c2 = np.mean(result[right])\n",
    "    err2 = np.sum((result[right] - c2) ** 2)\n",
    "\n",
    "    return err1 + err2\n",
    "\n",
    "\n",
    "def bestdivide(data, result, sequence, num_bins=256):\n",
    "    \"\"\"\n",
    "    Find the best splitting parameter and point using histogram-based binning.\n",
    "\n",
    "    :param data: numpy.ndarray\n",
    "        The dataset with samples as rows and features as columns.\n",
    "    :param result: numpy.ndarray\n",
    "        The target values corresponding to the data.\n",
    "    :param sequence: list or numpy.ndarray\n",
    "        Indices of the data points to consider for splitting.\n",
    "    :param num_bins: int, optional\n",
    "        Number of bins for histogram-based binning (default is 256).\n",
    "\n",
    "    :return min_para: int\n",
    "        The index of the best feature for splitting.\n",
    "    :return min_divide: float\n",
    "        The optimal split point for the feature.\n",
    "    :return min_error: float\n",
    "        The squared error for the optimal split.\n",
    "    \"\"\"\n",
    "    min_para = None\n",
    "    min_divide = None\n",
    "    min_error = float('inf')\n",
    "    \n",
    "    for para in range(data.shape[1]):\n",
    "        # Extract feature values for the current parameter\n",
    "        feature_values = data[sequence, para]\n",
    "        \n",
    "        # Compute histogram bins\n",
    "        bins = np.linspace(feature_values.min(), feature_values.max(), num_bins + 1)\n",
    "        digitized = np.digitize(feature_values, bins) - 1  # Bin indices\n",
    "        \n",
    "        # Bin boundaries as possible split points\n",
    "        bin_boundaries = (bins[:-1] + bins[1:]) / 2  # Midpoints between bin edges\n",
    "        \n",
    "        for boundary in bin_boundaries:\n",
    "            error = squaErr(data, result, sequence, para, boundary)\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                min_para = para\n",
    "                min_divide = boundary\n",
    "\n",
    "    return min_para, min_divide, min_error\n",
    "\n",
    "\n",
    "def squared_loss(predict, target):\n",
    "    '''\n",
    "    Calculates the sum of squared loss between predicted values and true values\n",
    "\n",
    "    :param predict: a 1-d numpy array containing the predicted values\n",
    "    :param target: a 1-d numpy array containing the true values\n",
    "    :return loss: squared loss\n",
    "    '''\n",
    "    loss = 0.5*np.sum(np.power(predict-target, 2))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Base Learner\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, min_samples=1, max_depth=3):\n",
    "        \"\"\"\n",
    "        Initialize the decision tree regressor.\n",
    "        :param min_samples: int, optional (default=1)\n",
    "            The minimum number of samples required to split a node.\n",
    "        :param max_depth: int, optional (default=3)\n",
    "            The maximum depth of the tree.\n",
    "        \"\"\"\n",
    "        self.min_samples = min_samples  # Minimum number of samples\n",
    "        self.max_depth = max_depth  # Maximum depth\n",
    "        self.root = None  # Root node of the decision tree\n",
    "\n",
    "    class RegressionTree:\n",
    "        def __init__(self, sequence, depth=0, max_depth=3):\n",
    "            \"\"\"\n",
    "            Initialize the regression tree node.\n",
    "            :param sequence: list of int\n",
    "                Indices of the samples at the current node.\n",
    "            :param depth: int, optional (default=0)\n",
    "                The depth of the current node in the tree.\n",
    "            :param max_depth: int, optional (default=3)\n",
    "                The maximum depth allowed for the tree.\n",
    "            \"\"\"\n",
    "            self.isLeaf = True  # Whether this node is a leaf\n",
    "            self.left = None  # Left subtree\n",
    "            self.right = None  # Right subtree\n",
    "            self.output = None  # Prediction value for the current node\n",
    "            self.sequence = sequence  # Indices of samples at the current node\n",
    "            self.parameter = None  # Splitting feature\n",
    "            self.divide = None  # Splitting point\n",
    "            self.depth = depth  # Current depth\n",
    "            self.max_depth = max_depth  # Maximum depth\n",
    "            self.leaf_index = id(self)  # Unique identifier for the leaf\n",
    "\n",
    "        # Grow from the current node\n",
    "        def grow(self, data, result, minnum):\n",
    "            \"\"\"\n",
    "            Performs recursive tree growth by finding the best split for \n",
    "            the data based on the given parameters. It stops if the sample size is \n",
    "            below a minimum threshold or the maximum tree depth is reached. The split \n",
    "            is based on minimizing the sum of squared errors.\n",
    "\n",
    "            :param data : numpy.ndarray\n",
    "                The dataset used to train the tree.\n",
    "            :param result : numpy.ndarray\n",
    "                The target values corresponding to the dataset.\n",
    "            :param minnum : int\n",
    "                The minimum number of samples required in a node to perform a split.\n",
    "            :return None\n",
    "            \"\"\"\n",
    "            if len(self.sequence) <= minnum or self.depth >= self.max_depth:  # Stop splitting if sample size is insufficient or maximum depth is reached\n",
    "                self.output = np.mean(result[self.sequence])  # Set the prediction value as the mean\n",
    "                return\n",
    "\n",
    "            # Find the best splitting feature and splitting point\n",
    "            parameter, divide, err = bestdivide(data, result, self.sequence)\n",
    "            left = []\n",
    "            right = []\n",
    "\n",
    "            # Split data\n",
    "            for i in self.sequence:\n",
    "                if data[i, parameter] < divide:\n",
    "                    left.append(i)\n",
    "                else:\n",
    "                    right.append(i)\n",
    "\n",
    "            # Update node information\n",
    "            self.parameter = parameter\n",
    "            self.divide = divide\n",
    "            self.isLeaf = False\n",
    "            self.left = DecisionTreeRegressor.RegressionTree(left, depth=self.depth + 1, max_depth=self.max_depth)\n",
    "            self.right = DecisionTreeRegressor.RegressionTree(right, depth=self.depth + 1, max_depth=self.max_depth)\n",
    "\n",
    "            # Recursively grow left and right subtrees\n",
    "            self.left.grow(data, result, minnum)\n",
    "            self.right.grow(data, result, minnum)\n",
    "\n",
    "        def predict_single(self, x):\n",
    "            \"\"\"\n",
    "            Traverses the decision tree and makes a prediction for a \n",
    "            single input sample. If the current node is a leaf, the prediction value \n",
    "            is returned. Otherwise, the method moves left or right based on the split.\n",
    "        \n",
    "            :param x : numpy.ndarray\n",
    "                A single sample for which the prediction is made.\n",
    "            :return self.right.predict_single(x): float\n",
    "                The predicted output for the input sample.\n",
    "            \"\"\"\n",
    "            if self.isLeaf:  # If this is a leaf node, return the prediction value\n",
    "                return self.output\n",
    "            if x[self.parameter] < self.divide:  # If less than the splitting point, go to the left subtree\n",
    "                return self.left.predict_single(x)\n",
    "            else:  # Otherwise, go to the right subtree\n",
    "                return self.right.predict_single(x)\n",
    "            \n",
    "        # Predict leaf index for a single sample\n",
    "        def predict_leaf_index_single(self, x):\n",
    "            \"\"\"\n",
    "            Traverses the decision tree and returns the index of the \n",
    "            leaf node where the sample would end up. If the current node is a leaf, \n",
    "            the leaf index is returned.\n",
    "\n",
    "            :param x : numpy.ndarray\n",
    "                A single sample for which the leaf index is predicted.\n",
    "            :return self.right.predict_leaf_index_single(x): int\n",
    "                The index of the leaf node for the input sample.\n",
    "            \"\"\"\n",
    "            if self.isLeaf:  # If this is a leaf node, return the leaf index\n",
    "                return self.leaf_index\n",
    "            if x[self.parameter] < self.divide:  # If less than the splitting point, go to the left subtree\n",
    "                return self.left.predict_leaf_index_single(x)\n",
    "            else:  # Otherwise, go to the right subtree\n",
    "                return self.right.predict_leaf_index_single(x)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model to the training data.\n",
    "        :param X: numpy.ndarray\n",
    "            The feature matrix (training data) where each row is a sample and each \n",
    "            column is a feature.\n",
    "        :param y: numpy.ndarray\n",
    "            The target values corresponding to the training data samples.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.root = self.RegressionTree(sequence=range(len(y)), max_depth=self.max_depth)\n",
    "        self.root.grow(X, y, self.min_samples)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the output for the input data.\n",
    "        :param X: numpy.ndarray\n",
    "            The input data for which predictions are to be made.\n",
    "        :return: numpy.ndarray\n",
    "            The predicted outputs for each sample in the input data.\n",
    "        \"\"\"\n",
    "        return np.array([self.root.predict_single(sample) for sample in X])\n",
    "    \n",
    "    def predict_leaf_indices(self, X):\n",
    "        \"\"\"\n",
    "        Predict the leaf indices for the input data.\n",
    "        :param X: numpy.ndarray\n",
    "            The input data for which leaf indices are to be predicted.\n",
    "        :return: numpy.ndarray\n",
    "            The indices of the leaf nodes for each sample in the input data.\n",
    "        \"\"\"\n",
    "        return np.array([self.root.predict_leaf_index_single(sample) for sample in X])\n",
    "    \n",
    "\n",
    "# Stochastic Gradient Boosting\n",
    "class StochasticGradientBoosting:\n",
    "    def __init__(self, learning_rate=0.1, n_estimators=20, subsample=0.5, min_samples=1, max_depth=3):\n",
    "        \"\"\"\n",
    "        :param learning_rate: learning rate, default 0.1\n",
    "        :param n_estimators: number of weak learners, default 20 (same as M)\n",
    "        :param subsample: fraction for subsampling, default 0.5\n",
    "        :param min_samples: the minimum number of samples for each tree, default 1\n",
    "        :param max_depth: the maximum depth for each tree (weak learner), default 3\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.subsample = subsample\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []    # models is a list of weak learners (decision trees)\n",
    "        self.gammas = []    # gammas is a list of lists of gamma value for each tree's region\n",
    "        self.initial_prediction = None  # will be initialized in train to be mean\n",
    "        self.leaf_indices_dict = []     # this will store a list of leaf indices for each tree\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train the Gradient Boosting model by iteratively fitting weak learners.\n",
    "        :param X: ndarray of shape (n_samples, n_features)\n",
    "            The input feature matrix for training.\n",
    "        :param Y: ndarray of shape (n_samples,)\n",
    "            The target values for training.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # initial guess F_m=mean(Y)\n",
    "        self.initial_prediction = np.mean(Y)\n",
    "        F_m = np.full(Y.shape, self.initial_prediction) # current F\n",
    "\n",
    "        for m in range(self.n_estimators):\n",
    "            # Random sampling for stochastic gradient boosting\n",
    "            batch_size = math.floor(self.subsample*len(Y))\n",
    "            indices = np.random.choice(len(Y), batch_size, replace=False)\n",
    "            X_batch, Y_batch = X[indices], Y[indices]\n",
    "\n",
    "            # Calculate residuals (negative gradient of the loss)\n",
    "            residuals = Y_batch - F_m[indices]\n",
    "\n",
    "            # Train a weak learner on the residuals\n",
    "            weak_learner = DecisionTreeRegressor(min_samples=self.min_samples, max_depth=self.max_depth)\n",
    "            weak_learner.fit(X_batch, residuals)\n",
    "            self.models.append(weak_learner)\n",
    "\n",
    "            # Update F_m for all samples\n",
    "            leaf_indices = weak_learner.predict_leaf_indices(X)\n",
    "            unique_leaves = np.unique(leaf_indices)\n",
    "            self.leaf_indices_dict.append(unique_leaves)\n",
    "            gamma_m = []    # the gammas for m'th tree, where each region (leaf) will have a corresponding gamma value\n",
    "            for leaf_index in unique_leaves:\n",
    "                region_mask = (leaf_indices == leaf_index)\n",
    "                gamma = np.mean(residuals[region_mask[indices]])\n",
    "                gamma_m.append(gamma)\n",
    "                F_m[region_mask] += self.learning_rate * gamma\n",
    "            self.gammas.append(gamma_m)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target values for the input data based on the trained model.\n",
    "        :param X: ndarray of shape (n_samples, n_features)\n",
    "            The input feature matrix for prediction.\n",
    "        :return F_m: ndarray of shape (n_samples,)\n",
    "            The predicted target values for the input samples.\n",
    "        \"\"\"\n",
    "        # Start with the initial prediction\n",
    "        F_m = np.full(X.shape[0], self.initial_prediction)\n",
    "\n",
    "        # Add contributions from each weak learner\n",
    "        for m, model in enumerate(self.models):\n",
    "            leaf_indices = model.predict_leaf_indices(X)\n",
    "            unique_leaves = self.leaf_indices_dict[m]\n",
    "            for i, leaf_index in enumerate(unique_leaves):\n",
    "                region_mask = (leaf_indices == leaf_index)\n",
    "                F_m[region_mask] += self.learning_rate * self.gammas[m][i]\n",
    "\n",
    "        return F_m\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        \"\"\"\n",
    "        Calculate the squared loss between predicted and true target values.\n",
    "        :param X: ndarray of shape (n_samples, n_features)\n",
    "            The input feature matrix.\n",
    "        :param Y: ndarray of shape (n_samples,)\n",
    "            The true target values.\n",
    "        :return loss: float\n",
    "            The squared loss between the predictions and the true target values.\n",
    "        \"\"\"\n",
    "        pred = self.predict(X)\n",
    "        loss = squared_loss(pred, Y)\n",
    "\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5216db-bee5-4455-ac86-a9c17da00f61",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba1e60-cd61-4a85-b8cc-8b69ae8661cf",
   "metadata": {},
   "source": [
    "Testing individual functions with dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a43a61aa-a20a-407a-a20b-cfbdac04fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "Y = np.array([1.1, 2.0, 2.9, 4.1, 5.0])\n",
    "\n",
    "model = StochasticGradientBoosting(\n",
    "    learning_rate=0.1, \n",
    "    n_estimators=10, \n",
    "    subsample=0.8, \n",
    "    min_samples=1, \n",
    "    max_depth=2\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train(X, Y)\n",
    "\n",
    "# Test initial prediction\n",
    "assert np.isclose(model.initial_prediction, np.mean(Y)), \"Initial prediction should be the mean of Y\"\n",
    "\n",
    "# Test the number of weak learners (decision trees) trained\n",
    "assert len(model.models) == model.n_estimators, \"Number of models should match n_estimators\"\n",
    "\n",
    "# Test that gammas have been computed for each tree\n",
    "assert len(model.gammas) == model.n_estimators, \"Gammas should be computed for each tree\"\n",
    "for gamma_list in model.gammas:\n",
    "    assert len(gamma_list) > 0, \"Each tree should have at least one gamma value\"\n",
    "\n",
    "# Test predictions\n",
    "predictions = model.predict(X)\n",
    "assert predictions.shape == Y.shape, \"Predictions should have the same shape as Y\"\n",
    "\n",
    "# Test if predictions improve with training\n",
    "assert np.mean((predictions - Y) ** 2) < np.mean((np.mean(Y) - Y) ** 2), \\\n",
    "    \"Predictions should reduce mean squared error compared to baseline\"\n",
    "\n",
    "# Test loss \n",
    "loss = model.loss(X, Y)\n",
    "assert loss >= 0, \"Loss should be non-negative\"\n",
    "expected_loss = 0.5 * np.sum(np.power(predictions - Y, 2)) \n",
    "assert np.isclose(loss, expected_loss), \"Loss should match the defined loss function (scaled SSE)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5f1e3-c212-440a-9187-f881a31ae29d",
   "metadata": {},
   "source": [
    "Testing against sklearn on dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a2ed4e-1539-4657-91a3-b175710520dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loss: 0.5607269200106376\n",
      "sklearn model loss: 0.38524973632554715\n",
      "model vs sklearn model: 0.1464148251085622\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# toy test for the whole model -- GradientBoostingRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "np.random.seed(1)\n",
    "# set some param for testing\n",
    "learning_rate=0.1\n",
    "n_estimators=50     # generally, more estimators lead to better resutls\n",
    "# subsample=0.5   # subsample of 1 achieves almost the same as sklearn, but subsample less than 1 causes some variation\n",
    "subsample=1\n",
    "min_samples=5\n",
    "max_depth=3\n",
    "\n",
    "def generate_synthetic_data(n_samples=100, noise=0.1):\n",
    "    X = np.random.rand(n_samples, 1) * 10 - 5  # Random features between [-5, 5]\n",
    "    Y = 2 * X.squeeze() + np.sin(X).squeeze() + np.random.randn(n_samples) * noise  # Linear + sine function + noise\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = generate_synthetic_data(n_samples=500)\n",
    "X_test, Y_test = generate_synthetic_data(n_samples=50)\n",
    "\n",
    "# Train the model\n",
    "model = StochasticGradientBoosting(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, min_samples=min_samples, max_depth=max_depth)\n",
    "model.train(X_train, Y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Also the sklearn model\n",
    "sklearn_model = GradientBoostingRegressor(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, min_samples_split=min_samples, max_depth=max_depth)\n",
    "sklearn_model.fit(X_train, Y_train)\n",
    "sklearn_pred = sklearn_model.predict(X_test)\n",
    "sklearn_loss = squared_loss(sklearn_pred, Y_test)\n",
    "\n",
    "# print loss\n",
    "print(f\"model loss: {squared_loss(predictions, Y_test)}\")\n",
    "print(f\"sklearn model loss: {squared_loss(sklearn_pred, Y_test)}\")\n",
    "print(f\"model vs sklearn model: {squared_loss(predictions, sklearn_pred)}\")\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_test, Y_test, color='blue', label='True values')\n",
    "plt.scatter(X_test, predictions, color='red', label='Predicted values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae37af-dfaf-4ece-961b-6418c8ebe3a0",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de09a55-30cf-4bea-8478-8de32950fd60",
   "metadata": {},
   "source": [
    "In this section, we are comparing our model with the sklearn GradientBoostingRegressor. Our goal is to test and compare their results on a public dataset to demonstrate that our model was able to reproduce the results of sklearn. Since the two models should work the same way, we will not add additional explanation of the sklearn model here, to read more about it please follow the link in the reference section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3f31b7-9538-4803-8b98-eba70ac242cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on wine.txt dataset\n",
      "----- Random State 0 -----\n",
      "----- StochasticGradientBoosting -----\n",
      "Random State 0: My Model Train Loss = 0.3800, Test Loss = 0.5691\n",
      "----- sklearn GradientBoostingRegressor -----\n",
      "Random State 0: Sklearn Model Train Loss = 0.3744, Test Loss = 0.5681\n",
      "----- Random State 1 -----\n",
      "----- StochasticGradientBoosting -----\n",
      "Random State 1: My Model Train Loss = 0.3917, Test Loss = 0.4786\n",
      "----- sklearn GradientBoostingRegressor -----\n",
      "Random State 1: Sklearn Model Train Loss = 0.3869, Test Loss = 0.4796\n",
      "----- Random State 2 -----\n",
      "----- StochasticGradientBoosting -----\n",
      "Random State 2: My Model Train Loss = 0.3893, Test Loss = 0.4795\n",
      "----- sklearn GradientBoostingRegressor -----\n",
      "Random State 2: Sklearn Model Train Loss = 0.3852, Test Loss = 0.4722\n",
      "----- Random State 3 -----\n",
      "----- StochasticGradientBoosting -----\n",
      "Random State 3: My Model Train Loss = 0.3788, Test Loss = 0.5201\n",
      "----- sklearn GradientBoostingRegressor -----\n",
      "Random State 3: Sklearn Model Train Loss = 0.3769, Test Loss = 0.5251\n",
      "----- Random State 4 -----\n",
      "----- StochasticGradientBoosting -----\n",
      "Random State 4: My Model Train Loss = 0.3899, Test Loss = 0.4709\n",
      "----- sklearn GradientBoostingRegressor -----\n",
      "Random State 4: Sklearn Model Train Loss = 0.3917, Test Loss = 0.4799\n",
      "----- Random State 5 -----\n",
      "----- StochasticGradientBoosting -----\n",
      "Random State 5: My Model Train Loss = 0.3944, Test Loss = 0.4472\n",
      "----- sklearn GradientBoostingRegressor -----\n",
      "Random State 5: Sklearn Model Train Loss = 0.3975, Test Loss = 0.4462\n",
      "----- Random State 6 -----\n",
      "----- StochasticGradientBoosting -----\n",
      "Random State 6: My Model Train Loss = 0.3918, Test Loss = 0.4710\n",
      "----- sklearn GradientBoostingRegressor -----\n",
      "Random State 6: Sklearn Model Train Loss = 0.3904, Test Loss = 0.4676\n",
      "----- Random State 7 -----\n",
      "----- StochasticGradientBoosting -----\n",
      "Random State 7: My Model Train Loss = 0.3870, Test Loss = 0.4781\n",
      "----- sklearn GradientBoostingRegressor -----\n",
      "Random State 7: Sklearn Model Train Loss = 0.3895, Test Loss = 0.4807\n",
      "----- Random State 8 -----\n",
      "----- StochasticGradientBoosting -----\n",
      "Random State 8: My Model Train Loss = 0.3917, Test Loss = 0.4624\n",
      "----- sklearn GradientBoostingRegressor -----\n",
      "Random State 8: Sklearn Model Train Loss = 0.3915, Test Loss = 0.4693\n",
      "----- Random State 9 -----\n",
      "----- StochasticGradientBoosting -----\n",
      "Random State 9: My Model Train Loss = 0.3854, Test Loss = 0.5214\n",
      "----- sklearn GradientBoostingRegressor -----\n",
      "Random State 9: Sklearn Model Train Loss = 0.3775, Test Loss = 0.5084\n",
      "----- Results Summary -----\n",
      "My Model:\n",
      "Training Loss - Mean: 0.3880, Std: 0.0049\n",
      "Testing Loss - Mean: 0.4898, Std: 0.0344\n",
      "Sklearn Model:\n",
      "Training Loss - Mean: 0.3861, Std: 0.0072\n",
      "Testing Loss - Mean: 0.4897, Std: 0.0334\n",
      "----- Comparison -----\n",
      "Training Loss Difference - Mean: 0.0019\n",
      "Testing Loss Difference - Mean: 0.0001\n",
      "Training Loss Difference - Std: -0.0023\n",
      "Testing Loss Difference - Std: 0.0011\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "\n",
    "\n",
    "def squared_loss(predictions, targets):\n",
    "    \"\"\"Custom squared loss calculation.\"\"\"\n",
    "    return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "def test_boosting_models(dataset, test_size=0.2, random_states=10):\n",
    "    \"\"\"\n",
    "    Compares the performance of our StochasticGradientBoosting with sklearn's GradientBoostingRegressor on a given dataset.\n",
    "    :param dataset: The path to the dataset\n",
    "    :param test_size: The proportion of the dataset to include in the test split\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dataset):\n",
    "        print(f\"The file {dataset} does not exist\")\n",
    "        exit()\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(dataset, skiprows=1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the features\n",
    "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "   \n",
    "    my_train_losses, my_test_losses = [], []\n",
    "    sklearn_train_losses, sklearn_test_losses = [], []\n",
    "\n",
    "    print('Running models on {} dataset'.format(dataset))\n",
    "\n",
    "    for i in range(random_states):\n",
    "        print(f\"----- Random State {i} -----\")\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=i)\n",
    "        #### StochasticGradientBoosting ######\n",
    "        print(\"----- StochasticGradientBoosting -----\")\n",
    "        my_model = StochasticGradientBoosting(\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=100,\n",
    "            subsample=0.8,\n",
    "            min_samples=2,\n",
    "            max_depth=3\n",
    "        )\n",
    "\n",
    "        my_model.train(X_train, Y_train)   \n",
    "        my_train_predictions = my_model.predict(X_train)\n",
    "        my_test_predictions = my_model.predict(X_test)\n",
    "    \n",
    "        my_train_loss = squared_loss(my_train_predictions, Y_train)\n",
    "        my_test_loss = squared_loss(my_test_predictions, Y_test)\n",
    "        print(f\"Random State {i}: My Model Train Loss = {my_train_loss:.4f}, Test Loss = {my_test_loss:.4f}\")\n",
    "        \n",
    "    \n",
    "        my_train_losses.append(my_train_loss)\n",
    "        my_test_losses.append(my_test_loss)\n",
    "       \n",
    "        #### sklearn GradientBoostingRegressor ######\n",
    "        print(\"----- sklearn GradientBoostingRegressor -----\")\n",
    "        sklearn_model = GradientBoostingRegressor(\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=100,\n",
    "            subsample=0.8,\n",
    "            min_samples_split=2,\n",
    "            max_depth=3,\n",
    "            random_state=0\n",
    "        )\n",
    "        sklearn_model.fit(X_train, Y_train)\n",
    "        sklearn_train_predictions = sklearn_model.predict(X_train)\n",
    "        sklearn_test_predictions = sklearn_model.predict(X_test)\n",
    "    \n",
    "        sklearn_train_loss = squared_loss(sklearn_train_predictions, Y_train)\n",
    "        sklearn_test_loss = squared_loss(sklearn_test_predictions, Y_test)\n",
    "        print(f\"Random State {i}: Sklearn Model Train Loss = {sklearn_train_loss:.4f}, Test Loss = {sklearn_test_loss:.4f}\")\n",
    "    \n",
    "        sklearn_train_losses.append(sklearn_train_loss)\n",
    "        sklearn_test_losses.append(sklearn_test_loss)\n",
    "    \n",
    "    #### Compare results ######\n",
    "    print(\"----- Results Summary -----\")\n",
    "    print(\"My Model:\")\n",
    "    print(f\"Training Loss - Mean: {np.mean(my_train_losses):.4f}, Std: {np.std(my_train_losses):.4f}\")\n",
    "    print(f\"Testing Loss - Mean: {np.mean(my_test_losses):.4f}, Std: {np.std(my_test_losses):.4f}\")\n",
    "    print(\"Sklearn Model:\")\n",
    "    print(f\"Training Loss - Mean: {np.mean(sklearn_train_losses):.4f}, Std: {np.std(sklearn_train_losses):.4f}\")\n",
    "    print(f\"Testing Loss - Mean: {np.mean(sklearn_test_losses):.4f}, Std: {np.std(sklearn_test_losses):.4f}\")\n",
    "\n",
    "    print(\"----- Comparison -----\")\n",
    "    print(f\"Training Loss Difference - Mean: {np.mean(my_train_losses) - np.mean(sklearn_train_losses):.4f}\")\n",
    "    print(f\"Testing Loss Difference - Mean: {np.mean(my_test_losses) - np.mean(sklearn_test_losses):.4f}\")\n",
    "    print(f\"Training Loss Difference - Std: {np.std(my_train_losses) - np.std(sklearn_train_losses):.4f}\")\n",
    "    print(f\"Testing Loss Difference - Std: {np.std(my_test_losses) - np.std(sklearn_test_losses):.4f}\")\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "test_boosting_models('wine.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ef56fb",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Friedman, J.H., 2002. Stochastic gradient boosting. Computational statistics & data analysis, 38(4), pp.367-378.\n",
    "2. Scikit-learn documentation: GradientBoostingRegressor. Available at: https://scikit-learn.org/dev/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html [Accessed 1 Dec. 2024]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c93604-964b-4b2a-ad9c-0aaae43e040e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
