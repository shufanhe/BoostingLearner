{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb5270b",
   "metadata": {},
   "source": [
    "# Overview: Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391d7c6-1c15-4528-8575-8b3aea1eb1b4",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Our goal is to find a function $F^*(\\mathbf{x})$ that maps $x$ to $y$ such that loss function $\\Psi(y, F(\\mathbf{x}))$ is minimized over the joint distribution of all $(y, x)$ values\n",
    "\n",
    "$$F^*(\\mathbf{x}) = \\arg \\min_{F(\\mathbf{x})} E_{y, \\mathbf{x}} \\, \\Psi(y, F(\\mathbf{x}))$$\n",
    "\n",
    "\n",
    "Boosting: additive expansion with base learner $h(\\mathbf{x}; \\mathbf{a}_m)$\n",
    "$$F(\\mathbf{x}) = \\sum_{m=0}^{M} \\beta_m h(\\mathbf{x}; \\mathbf{a}_m)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94572767-125c-4697-82a8-9a830bef32da",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting\n",
    "\n",
    "$$F_0(\\mathbf{x}) = \\arg \\min_{\\gamma} \\sum_{i=1}^{N} \\Psi(y_i, \\gamma)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebc0d5",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Tree Boosting Pseudocode**\n",
    "- Init input\n",
    "    - learning_rate = 0.1\n",
    "    - n_esitmators = 20                 \n",
    "        - this is denoted as M in the training pseudocode below\n",
    "    - subsample = 0.5    \n",
    "        - fraction for subsampling, if subsample<1, it is stochastic\n",
    "    - min_samples = 1\n",
    "        - the minimum number of samples to split for each tree\n",
    "    - max_depth = 3                              \n",
    "        - the maximum depth for each tree\n",
    "\n",
    "- Train(X, Y)\n",
    "    - start with an initial guess, say F(x) = mean(Y)\n",
    "    - for m = 1 to M do:\n",
    "        - X_batch, Y_batch = random_sampling(X, Y, fraction for subsampling (batch size))    # fraction < 1 leads to stochastic gradient boosting\n",
    "        - Y_residual = negative gradient of loss between (true) Y and current F based on the batch\n",
    "            - since we use (half of) sum of squared loss here\n",
    "            - the gradient is thus F_m_batch-Y_batch\n",
    "        - weak learner h(x) = new weakLearner.fit(X_batch, Y_residual)\n",
    "        - weak learner Tree $\\{R_{lm}\\}_1^L$ = L-terminal node tree fitting on (X_batch, Y_residual)\n",
    "            - here $\\{R_{lm}\\}_1^L$ represents a set of L regions (leaf nodes) in stage m\n",
    "            - $R_{lm}$ represents the l-th region (leaf node) in stage m\n",
    "        - $\\gamma_{lm} = arg min_{\\gamma} \\sum_{x-batch \\in R_{lm}}$ loss(Y_batch, F(x)+$\\gamma$)\n",
    "            - that is, for each region l at stage m, find $\\gamma$ such that it minimizes the loss for the new F(x) = F(x) + $\\gamma$\n",
    "            - which is the mean of the sample values at region l at stage m\n",
    "        - update F(x) = F(x) + learning_rate * $\\gamma * 1(x \\in R_{lm})$\n",
    "            - that is, F(x) = F(x) + learning_rate * new tree\n",
    "            - note here we are multiplying the new tree with learning rate (different from how we obtain the best $\\gamma$)\n",
    "    - end for\n",
    "\n",
    "- Predict(X)\n",
    "    - Given input X, predict their values\n",
    "    - which is the sum of initial guess and all the weak learners' prediction\n",
    "    - where each weak learner's prediction is learning_rate * $\\gamma * 1(x \\in R_{lm})$\n",
    "\n",
    "- Loss(X, Y)\n",
    "    - Given input examples and their true values, calculate the squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77f72ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the sum of squared errors for the chosen parameter and splitting point\n",
    "def squaErr(data, result, sequence, parameter, divide):\n",
    "    left = []\n",
    "    right = []\n",
    "\n",
    "    for i in sequence:\n",
    "        if data[i, parameter] < divide:\n",
    "            left.append(i)\n",
    "        else:\n",
    "            right.append(i)\n",
    "\n",
    "    if len(left) == 0 or len(right) == 0:  # If either subset is empty, return positive infinity\n",
    "        return float('inf')\n",
    "\n",
    "    c1 = np.mean(result[left])\n",
    "    err1 = np.sum((result[left] - c1) ** 2)\n",
    "\n",
    "    c2 = np.mean(result[right])\n",
    "    err2 = np.sum((result[right] - c2) ** 2)\n",
    "\n",
    "    return err1 + err2\n",
    "\n",
    "# Determine the next splitting parameter and splitting point using histogram-based binning\n",
    "def bestdivide(data, result, sequence, num_bins=256):\n",
    "    min_para = None\n",
    "    min_divide = None\n",
    "    min_error = float('inf')\n",
    "    \n",
    "    for para in range(data.shape[1]):\n",
    "        # Extract feature values for the current parameter\n",
    "        feature_values = data[sequence, para]\n",
    "        \n",
    "        # Compute histogram bins\n",
    "        bins = np.linspace(feature_values.min(), feature_values.max(), num_bins + 1)\n",
    "        digitized = np.digitize(feature_values, bins) - 1  # Bin indices\n",
    "        \n",
    "        # Bin boundaries as possible split points\n",
    "        bin_boundaries = (bins[:-1] + bins[1:]) / 2  # Midpoints between bin edges\n",
    "        \n",
    "        for boundary in bin_boundaries:\n",
    "            error = squaErr(data, result, sequence, para, boundary)\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                min_para = para\n",
    "                min_divide = boundary\n",
    "\n",
    "    return min_para, min_divide, min_error\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, min_samples=1, max_depth=3):\n",
    "        self.min_samples = min_samples  # Minimum number of samples\n",
    "        self.max_depth = max_depth  # Maximum depth\n",
    "        self.root = None  # Root node of the decision tree\n",
    "\n",
    "    class RegressionTree:\n",
    "        def __init__(self, sequence, depth=0, max_depth=3):\n",
    "            self.isLeaf = True  # Whether this node is a leaf\n",
    "            self.left = None  # Left subtree\n",
    "            self.right = None  # Right subtree\n",
    "            self.output = None  # Prediction value for the current node\n",
    "            self.sequence = sequence  # Indices of samples at the current node\n",
    "            self.parameter = None  # Splitting feature\n",
    "            self.divide = None  # Splitting point\n",
    "            self.depth = depth  # Current depth\n",
    "            self.max_depth = max_depth  # Maximum depth\n",
    "            self.leaf_index = id(self)  # Unique identifier for the leaf\n",
    "\n",
    "        # Grow from the current node\n",
    "        def grow(self, data, result, minnum):\n",
    "            if len(self.sequence) <= minnum or self.depth >= self.max_depth:  # Stop splitting if sample size is insufficient or maximum depth is reached\n",
    "                self.output = np.mean(result[self.sequence])  # Set the prediction value as the mean\n",
    "                return\n",
    "\n",
    "            # Find the best splitting feature and splitting point\n",
    "            parameter, divide, err = bestdivide(data, result, self.sequence)\n",
    "            left = []\n",
    "            right = []\n",
    "\n",
    "            # Split data\n",
    "            for i in self.sequence:\n",
    "                if data[i, parameter] < divide:\n",
    "                    left.append(i)\n",
    "                else:\n",
    "                    right.append(i)\n",
    "\n",
    "            # Update node information\n",
    "            self.parameter = parameter\n",
    "            self.divide = divide\n",
    "            self.isLeaf = False\n",
    "            self.left = DecisionTreeRegressor.RegressionTree(left, depth=self.depth + 1, max_depth=self.max_depth)\n",
    "            self.right = DecisionTreeRegressor.RegressionTree(right, depth=self.depth + 1, max_depth=self.max_depth)\n",
    "\n",
    "            # Recursively grow left and right subtrees\n",
    "            self.left.grow(data, result, minnum)\n",
    "            self.right.grow(data, result, minnum)\n",
    "\n",
    "        # Predict a single sample\n",
    "        def predict_single(self, x):\n",
    "            if self.isLeaf:  # If this is a leaf node, return the prediction value\n",
    "                return self.output\n",
    "            if x[self.parameter] < self.divide:  # If less than the splitting point, go to the left subtree\n",
    "                return self.left.predict_single(x)\n",
    "            else:  # Otherwise, go to the right subtree\n",
    "                return self.right.predict_single(x)\n",
    "            \n",
    "        # Predict leaf index for a single sample\n",
    "        def predict_leaf_index_single(self, x):\n",
    "            if self.isLeaf:  # If this is a leaf node, return the leaf index\n",
    "                return self.leaf_index\n",
    "            if x[self.parameter] < self.divide:  # If less than the splitting point, go to the left subtree\n",
    "                return self.left.predict_leaf_index_single(x)\n",
    "            else:  # Otherwise, go to the right subtree\n",
    "                return self.right.predict_leaf_index_single(x)\n",
    "\n",
    "    # Fit the training data\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.RegressionTree(sequence=range(len(y)), max_depth=self.max_depth)\n",
    "        self.root.grow(X, y, self.min_samples)\n",
    "\n",
    "    # Predict the input data\n",
    "    def predict(self, X):\n",
    "        return np.array([self.root.predict_single(sample) for sample in X])\n",
    "    \n",
    "    # Predict leaf indices for the input data\n",
    "    def predict_leaf_indices(self, X):\n",
    "        return np.array([self.root.predict_leaf_index_single(sample) for sample in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "113e4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def squared_loss(predict, target):\n",
    "    '''\n",
    "    Calculates the sum of squared loss between predicted values and true values\n",
    "\n",
    "    :param predict: a 1-d numpy array containing the predicted values\n",
    "    :param target: a 1-d numpy array containing the true values\n",
    "    :return loss: squared loss\n",
    "    '''\n",
    "    loss = 0.5*np.sum(np.power(predict-target, 2))\n",
    "\n",
    "    return loss\n",
    "\n",
    "class StochasticGradientBoosting:\n",
    "    def __init__(self, learning_rate=0.1, n_estimators=20, subsample=0.5, min_samples=1, max_depth=3):\n",
    "        \"\"\"\n",
    "        :param learning_rate: learning rate, default 0.1\n",
    "        :param n_estimators: number of weak learners, default 20 (same as M)\n",
    "        :param subsample: fraction for subsampling, default 0.5\n",
    "        :param min_samples: the minimum number of samples for each tree, default 1\n",
    "        :param max_depth: the maximum depth for each tree (weak learner), default 3\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.subsample = subsample\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []    # models is a list of weak learners (decision trees)\n",
    "        self.gammas = []    # gammas is a list of lists of gamma value for each tree's region\n",
    "        self.initial_prediction = None  # will be initialized in train to be mean\n",
    "        self.leaf_indices_dict = []     # this will store a list of leaf indices for each tree\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        # initial guess F_m=mean(Y)\n",
    "        self.initial_prediction = np.mean(Y)\n",
    "        F_m = np.full(Y.shape, self.initial_prediction) # current F\n",
    "\n",
    "        for m in range(self.n_estimators):\n",
    "            # Random sampling for stochastic gradient boosting\n",
    "            batch_size = math.floor(self.subsample*len(Y))\n",
    "            indices = np.random.choice(len(Y), batch_size, replace=False)\n",
    "            X_batch, Y_batch = X[indices], Y[indices]\n",
    "\n",
    "            # Calculate residuals (negative gradient of the loss)\n",
    "            residuals = Y_batch - F_m[indices]\n",
    "\n",
    "            # Train a weak learner on the residuals\n",
    "            weak_learner = DecisionTreeRegressor(min_samples=self.min_samples, max_depth=self.max_depth)\n",
    "            weak_learner.fit(X_batch, residuals)\n",
    "            self.models.append(weak_learner)\n",
    "\n",
    "            # Update F_m for all samples\n",
    "            leaf_indices = weak_learner.predict_leaf_indices(X)\n",
    "            unique_leaves = np.unique(leaf_indices)\n",
    "            self.leaf_indices_dict.append(unique_leaves)\n",
    "            gamma_m = []    # the gammas for m'th tree, where each region (leaf) will have a corresponding gamma value\n",
    "            for leaf_index in unique_leaves:\n",
    "                region_mask = (leaf_indices == leaf_index)\n",
    "                gamma = np.mean(residuals[region_mask[indices]])\n",
    "                gamma_m.append(gamma)\n",
    "                F_m[region_mask] += self.learning_rate * gamma\n",
    "            self.gammas.append(gamma_m)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Start with the initial prediction\n",
    "        F_m = np.full(X.shape[0], self.initial_prediction)\n",
    "\n",
    "        # Add contributions from each weak learner\n",
    "        for m, model in enumerate(self.models):\n",
    "            leaf_indices = model.predict_leaf_indices(X)\n",
    "            unique_leaves = self.leaf_indices_dict[m]\n",
    "            for i, leaf_index in enumerate(unique_leaves):\n",
    "                region_mask = (leaf_indices == leaf_index)\n",
    "                F_m[region_mask] += self.learning_rate * self.gammas[m][i]\n",
    "\n",
    "        return F_m\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        \"\"\"calculates the squared loss given inputs and their true values\"\"\"\n",
    "        pred = self.predict(X)\n",
    "        loss = squared_loss(pred, Y)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5216db-bee5-4455-ac86-a9c17da00f61",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba1e60-cd61-4a85-b8cc-8b69ae8661cf",
   "metadata": {},
   "source": [
    "Testing individual functions with dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a43a61aa-a20a-407a-a20b-cfbdac04fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "Y = np.array([1.1, 2.0, 2.9, 4.1, 5.0])\n",
    "\n",
    "model = StochasticGradientBoosting(\n",
    "    learning_rate=0.1, \n",
    "    n_estimators=10, \n",
    "    subsample=0.8, \n",
    "    min_samples=1, \n",
    "    max_depth=2\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train(X, Y)\n",
    "\n",
    "# Test initial prediction\n",
    "assert np.isclose(model.initial_prediction, np.mean(Y)), \"Initial prediction should be the mean of Y\"\n",
    "\n",
    "# Test the number of weak learners (decision trees) trained\n",
    "assert len(model.models) == model.n_estimators, \"Number of models should match n_estimators\"\n",
    "\n",
    "# Test that gammas have been computed for each tree\n",
    "assert len(model.gammas) == model.n_estimators, \"Gammas should be computed for each tree\"\n",
    "for gamma_list in model.gammas:\n",
    "    assert len(gamma_list) > 0, \"Each tree should have at least one gamma value\"\n",
    "\n",
    "# Test predictions\n",
    "predictions = model.predict(X)\n",
    "assert predictions.shape == Y.shape, \"Predictions should have the same shape as Y\"\n",
    "\n",
    "# Test if predictions improve with training\n",
    "assert np.mean((predictions - Y) ** 2) < np.mean((np.mean(Y) - Y) ** 2), \\\n",
    "    \"Predictions should reduce mean squared error compared to baseline\"\n",
    "\n",
    "# Test loss \n",
    "loss = model.loss(X, Y)\n",
    "assert loss >= 0, \"Loss should be non-negative\"\n",
    "expected_loss = 0.5 * np.sum(np.power(predictions - Y, 2)) # or mse?\n",
    "assert np.isclose(loss, expected_loss), \"Loss should match the defined loss function (scaled SSE)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5f1e3-c212-440a-9187-f881a31ae29d",
   "metadata": {},
   "source": [
    "Testing against sklearn on dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a2ed4e-1539-4657-91a3-b175710520dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loss: 0.5607269200106376\n",
      "sklearn model loss: 0.38524973632554715\n",
      "model vs sklearn model: 0.1464148251085622\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# toy test for the whole model -- GradientBoostingRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "np.random.seed(1)\n",
    "# set some param for testing\n",
    "learning_rate=0.1\n",
    "n_estimators=50     # generally, more estimators lead to better resutls\n",
    "# subsample=0.5   # subsample of 1 achieves almost the same as sklearn, but subsample less than 1 causes some variation\n",
    "subsample=1\n",
    "min_samples=5\n",
    "max_depth=3\n",
    "\n",
    "def generate_synthetic_data(n_samples=100, noise=0.1):\n",
    "    X = np.random.rand(n_samples, 1) * 10 - 5  # Random features between [-5, 5]\n",
    "    Y = 2 * X.squeeze() + np.sin(X).squeeze() + np.random.randn(n_samples) * noise  # Linear + sine function + noise\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = generate_synthetic_data(n_samples=500)\n",
    "X_test, Y_test = generate_synthetic_data(n_samples=50)\n",
    "\n",
    "# Train the model\n",
    "model = StochasticGradientBoosting(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, min_samples=min_samples, max_depth=max_depth)\n",
    "model.train(X_train, Y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Also the sklearn model\n",
    "sklearn_model = GradientBoostingRegressor(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, min_samples_split=min_samples, max_depth=max_depth)\n",
    "sklearn_model.fit(X_train, Y_train)\n",
    "sklearn_pred = sklearn_model.predict(X_test)\n",
    "sklearn_loss = squared_loss(sklearn_pred, Y_test)\n",
    "\n",
    "# print loss\n",
    "print(f\"model loss: {squared_loss(predictions, Y_test)}\")\n",
    "print(f\"sklearn model loss: {squared_loss(sklearn_pred, Y_test)}\")\n",
    "print(f\"model vs sklearn model: {squared_loss(predictions, sklearn_pred)}\")\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_test, Y_test, color='blue', label='True values')\n",
    "plt.scatter(X_test, predictions, color='red', label='Predicted values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae37af-dfaf-4ece-961b-6418c8ebe3a0",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f3f31b7-9538-4803-8b98-eba70ac242cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on wine.txt dataset\n",
      "----- StochasticGradientBoosting -----\n",
      "hi\n",
      "My Model Training Loss: 0.378544231509949\n",
      "My Model Testing Loss: 0.5809187798022113\n",
      "----- sklearn GradientBoostingRegressor -----\n",
      "Sklearn Model Training Loss: 0.37438377629098263\n",
      "Sklearn Model Testing Loss: 0.568075903436917\n",
      "----- Comparison -----\n",
      "Training Loss Difference: 0.0042\n",
      "Testing Loss Difference: 0.0128\n"
     ]
    }
   ],
   "source": [
    "# get a public dataset\n",
    "\n",
    "\n",
    "# compare model result on the dataset with sklearn result on the same dataset\n",
    "#X, Y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "\n",
    "\n",
    "def squared_loss(predictions, targets):\n",
    "    \"\"\"Custom squared loss calculation.\"\"\"\n",
    "    return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "def test_boosting_models(dataset, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Compares the performance of our StochasticGradientBoosting with sklearn's GradientBoostingRegressor on a given dataset.\n",
    "    :param dataset: The path to the dataset\n",
    "    :param test_size: The proportion of the dataset to include in the test split\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dataset):\n",
    "        print(f\"The file {dataset} does not exist\")\n",
    "        exit()\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(dataset, skiprows=1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the features\n",
    "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "\n",
    "    print('Running models on {} dataset'.format(dataset))\n",
    "    \n",
    "    #### StochasticGradientBoosting ######\n",
    "    print(\"----- StochasticGradientBoosting -----\")\n",
    "    my_model = StochasticGradientBoosting(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        subsample=0.8,\n",
    "        min_samples=2,\n",
    "        max_depth=3\n",
    "    )\n",
    "    #print(X_train.shape, Y_train.shape)\n",
    "    my_model.train(X_train, Y_train)   # why is this taking so long\n",
    "    print(\"hi\")\n",
    "    my_train_predictions = my_model.predict(X_train)\n",
    "    my_test_predictions = my_model.predict(X_test)\n",
    "\n",
    "    my_train_loss = squared_loss(my_train_predictions, Y_train)\n",
    "    my_test_loss = squared_loss(my_test_predictions, Y_test)\n",
    "\n",
    "    print(\"My Model Training Loss:\", my_train_loss)\n",
    "    print(\"My Model Testing Loss:\", my_test_loss)\n",
    "   \n",
    "    #### sklearn GradientBoostingRegressor ######\n",
    "    print(\"----- sklearn GradientBoostingRegressor -----\")\n",
    "    sklearn_model = GradientBoostingRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        subsample=0.8,\n",
    "        min_samples_split=2,\n",
    "        max_depth=3,\n",
    "        random_state=0\n",
    "    )\n",
    "    sklearn_model.fit(X_train, Y_train)\n",
    "    sklearn_train_predictions = sklearn_model.predict(X_train)\n",
    "    sklearn_test_predictions = sklearn_model.predict(X_test)\n",
    "\n",
    "    sklearn_train_loss = squared_loss(sklearn_train_predictions, Y_train)\n",
    "    sklearn_test_loss = squared_loss(sklearn_test_predictions, Y_test)\n",
    "\n",
    "    print(\"Sklearn Model Training Loss:\", sklearn_train_loss)\n",
    "    print(\"Sklearn Model Testing Loss:\", sklearn_test_loss)\n",
    "\n",
    "    #### Compare results ######\n",
    "    print(\"----- Comparison -----\")\n",
    "    print(f\"Training Loss Difference: {my_train_loss - sklearn_train_loss:.4f}\")\n",
    "    print(f\"Testing Loss Difference: {my_test_loss - sklearn_test_loss:.4f}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "test_boosting_models('wine.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c93604-964b-4b2a-ad9c-0aaae43e040e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbc375-c30a-41b3-9995-b951c0bd4b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
